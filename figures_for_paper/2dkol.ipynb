{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import h5py\n",
    "import yaml\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../flowrec/utils/a4.mplstyle')\n",
    "\n",
    "from pathlib import Path\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from flowrec.utils import my_discrete_cmap\n",
    "from flowrec.utils.system import set_gpu\n",
    "from flowrec.losses import relative_error, momentum_loss, divergence\n",
    "from flowrec.physics_and_derivatives import vorticity, get_tke\n",
    "from flowrec.utils.simulation import kolsol_forcing_term\n",
    "set_gpu(1)\n",
    "grey = '#808080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define shared functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowrec.sensors import griddata_periodic\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def interpolate(insert_observation_fn, sensor_locs, data_shape, observed):\n",
    "\n",
    "    sensors_loc_x, sensors_loc_y = sensor_locs\n",
    "\n",
    "    compare_interp = []\n",
    "    nt = data_shape[0]\n",
    "    ndim = data_shape[-1]\n",
    "\n",
    "    side_length = data_shape[1]\n",
    "    g1,g2 = np.mgrid[-side_length:side_length*2, -side_length:side_length*2]\n",
    "    \n",
    "    temp_observed = np.empty(data_shape)\n",
    "    temp_observed.fill(np.nan) #this is noisy\n",
    "    temp_observed = insert_observation_fn(jnp.asarray(temp_observed),jnp.asarray(observed)) # observed_test is noisy if\n",
    "\n",
    "    for i in range(ndim):\n",
    "        _locs = np.stack((sensors_loc_x[i].flatten(),sensors_loc_y[i].flatten()),axis=1)\n",
    "        for t in range(nt):\n",
    "            _interp = griddata_periodic(_locs,temp_observed[t,...,i][~np.isnan(temp_observed[t,...,i])],(g1,g2),'cubic',side_length)\n",
    "            compare_interp.append(_interp[side_length:2*side_length,side_length:2*side_length])\n",
    "\n",
    "    compare_interp = np.array(compare_interp)\n",
    "    if ndim > 1:\n",
    "        compare_interp = np.stack((compare_interp[:nt,...],compare_interp[nt:2*nt,...],compare_interp[2*nt:3*nt,...]),axis=-1)\n",
    "\n",
    "    return compare_interp, temp_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensor_locs(example_train, take_observation_fn, insert_observation_fn):\n",
    "\n",
    "    sensors_empty = np.empty_like(example_train[[0],...])\n",
    "    sensors_empty.fill(np.nan)\n",
    "    grid_x, grid_y = np.mgrid[0:example_train.shape[1], 0:example_train.shape[2]]\n",
    "\n",
    "    gridx1 = np.repeat(grid_x[None,:,:,None],3,axis=3)\n",
    "    gridy1 = np.repeat(grid_y[None,:,:,None],3,axis=3)\n",
    "\n",
    "    idx_x = take_observation_fn(gridx1)\n",
    "    idx_y = take_observation_fn(gridy1)\n",
    "\n",
    "    idx_x = insert_observation_fn(jnp.asarray(sensors_empty),jnp.asarray(idx_x))[0,...]\n",
    "    sensors_loc_x = []\n",
    "    for i in range(idx_x.shape[-1]):\n",
    "        sensors_loc_x.append(idx_x[...,i][~np.isnan(idx_x[...,i])].astype(int))\n",
    "\n",
    "    idx_y = insert_observation_fn(jnp.asarray(sensors_empty),jnp.asarray(idx_y))[0,...]\n",
    "    sensors_loc_y = []\n",
    "    for i in range(idx_y.shape[-1]):\n",
    "        sensors_loc_y.append(idx_y[...,i][~np.isnan(idx_y[...,i])].astype(int))\n",
    "    \n",
    "    return [sensors_loc_x, sensors_loc_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import flowrec.training_and_states as state_utils\n",
    "import flowrec.data as data_utils\n",
    "def get_summary_onecase(d, predict_only=False):\n",
    "\n",
    "    with open(Path(d,'config.yml'), 'r') as f:\n",
    "        cfg = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "    cfg.data_config.update({'data_dir':'.'+cfg.data_config.data_dir})\n",
    "    datacfg = cfg.data_config\n",
    "    mdlcfg = cfg.model_config\n",
    "    traincfg = cfg.train_config\n",
    "\n",
    "    print('Loading data')\n",
    "    data, datainfo = cfg.case.dataloader(datacfg)\n",
    "\n",
    "    _keys_to_exclude = [\n",
    "        'u_train_clean',\n",
    "        'u_val_clean',\n",
    "        'train_minmax',\n",
    "        'val_minmax',\n",
    "        'u_train',\n",
    "        'u_val',\n",
    "        'inn_train',\n",
    "        'inn_val'\n",
    "    ]\n",
    "    observe_kwargs = {key: value for key, value in data.items() if key not in _keys_to_exclude}\n",
    "    print('Building observation functions')\n",
    "    take_observation, insert_observation = cfg.case.observe(\n",
    "        datacfg,\n",
    "        example_pred_snapshot = data['u_train'][0,...],\n",
    "        example_pin_snapshot = data['inn_train'][0,...],\n",
    "        **observe_kwargs\n",
    "    )\n",
    "    observed_train, train_minmax = take_observation(data['u_train'], init=True)\n",
    "    observed_val, val_minmax = take_observation(data['u_val'], init=True)\n",
    "    data.update({\n",
    "        'y_train':observed_train, # not normalised\n",
    "        'y_val':observed_val, # not normalised\n",
    "        'train_minmax':train_minmax,\n",
    "        'val_minmax':val_minmax \n",
    "    })\n",
    "    print('Building model')\n",
    "    prep_data, make_model = cfg.case.select_model(datacfg = datacfg, mdlcfg = mdlcfg, traincfg = traincfg)\n",
    "    data = prep_data(data, datainfo)\n",
    "    mdl = make_model(mdlcfg)\n",
    "    state = state_utils.restore_trainingstate(d,'state')\n",
    "    inn_train = data['inn_train']\n",
    "    if datacfg.snr:\n",
    "        yfull_train_clean = data['u_train_clean']\n",
    "    else:\n",
    "        yfull_train_clean = data['u_train']\n",
    "\n",
    "    print('Predicting')\n",
    "    pred_train = []\n",
    "    _t = 0\n",
    "    while _t < inn_train.shape[0]:\n",
    "        if (_t + 500) < inn_train.shape[0]:\n",
    "            pred_train.append(\n",
    "                mdl.predict(state.params, inn_train[_t:_t+500,:])\n",
    "            )\n",
    "        else:\n",
    "            pred_train.append(\n",
    "                mdl.predict(state.params, inn_train[_t:,:])\n",
    "            )\n",
    "        _t = _t + 500\n",
    "\n",
    "    pred_train = np.concatenate(pred_train, axis=0)\n",
    "\n",
    "    if cfg.data_config.normalise:\n",
    "        pred_train = data_utils.unnormalise_group(pred_train, train_minmax, axis_data=-1, axis_range=0)\n",
    "\n",
    "    if predict_only:\n",
    "        return pred_train\n",
    "    else:\n",
    "        sensor_locs = get_sensor_locs(yfull_train_clean[:5,...], take_observation, insert_observation)\n",
    "\n",
    "        print('Interpolating from observations')\n",
    "        u_interp, observed = interpolate(insert_observation, sensor_locs, yfull_train_clean.shape, observed_train)\n",
    "    \n",
    "        # return (clean, noisy, interp, predicted)\n",
    "        return (yfull_train_clean, data['u_train'], u_interp, pred_train), datainfo, observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_sensor_locations(results, observed, t1, figname):\n",
    "    fig = plt.figure(figsize=(2,2))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(results[0][t1,...,-1].T, alpha=0.3, zorder=1)\n",
    "    ax.spy(observed[t1,...,-1], color='r', marker='s', markersize=2, alpha=0.6, zorder=2)\n",
    "    ax.spy(observed[t1,...,0], color='k', marker='s', markersize=2, zorder=5)\n",
    "    ax.set(xticks=[], xlabel='$x_1$', ylabel='$x_2$', yticks=[])\n",
    "    if figname:\n",
    "        fig.savefig('./figs/'+figname)\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_tke(ufluc, datainfo):\n",
    "\n",
    "#     k = np.fft.fftfreq(128,d=datainfo.dx)\n",
    "#     dk = k[3]-k[2]\n",
    "#     k = np.fft.fftfreq(128,d=1/128)\n",
    "#     kgrid1,kgrid2 = np.meshgrid(k,k)\n",
    "#     kgrid_magnitude = np.sqrt((kgrid1**2)+(kgrid2**2))\n",
    "#     kgrid_magnitude_int = kgrid_magnitude.astype('int')\n",
    "#     kmax = np.max(kgrid_magnitude_int)\n",
    "#     kbins = np.arange(kmax).astype('int')\n",
    "\n",
    "\n",
    "#     u_fft = np.fft.fft2(ufluc[...,:2],axes=[1,2])\n",
    "#     ke_fft = np.sum(u_fft * np.conj(u_fft),axis=-1).real * 0.5\n",
    "#     ke_avg = np.mean(ke_fft,axis=0)\n",
    "#     spectrum = np.zeros_like(kbins).astype('float32')\n",
    "#     for i in kbins:\n",
    "#         spectrum[i] += 0.5*np.sum(ke_avg[kgrid_magnitude_int==i])\n",
    "    \n",
    "#     return spectrum, kbins, dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = re.compile('^3-')\n",
    "pmean3 = re.compile('^mean3-')\n",
    "pclassic = re.compile('^classic-')\n",
    "\n",
    "def is_loss3(name):\n",
    "    if re.search(p3, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_lossmean(name):\n",
    "    if re.search(pmean3, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_lossclassic(name):\n",
    "    if re.search(pclassic, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot summary compare loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../local_results/2dkol/repeat_clean_minimum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary_clean_all(d):\n",
    "\n",
    "    with h5py.File(Path(d,'summary.h5')) as hf:\n",
    "        names = np.array(hf.get('runs_name')).astype('unicode')\n",
    "        l_train = np.array(hf.get('runs_loss_train'))\n",
    "\n",
    "    loss_total_loss3 = []\n",
    "    loss_rel_loss3 = []\n",
    "    name_loss3 = []\n",
    "    loss_total_mean3 = []\n",
    "    loss_rel_mean3 = []\n",
    "    name_mean3 = []\n",
    "    loss_total_classic = []\n",
    "    loss_rel_classic = []\n",
    "    name_classic = []\n",
    "    for i in range(len(names)):\n",
    "        if is_loss3(names[i]):\n",
    "            name_loss3.append(names[i])\n",
    "            loss_total_loss3.append(np.sum(l_train[i,1:]))\n",
    "            loss_rel_loss3.append(l_train[i,0])\n",
    "        if is_lossmean(names[i]):\n",
    "            name_mean3.append(names[i])\n",
    "            loss_total_mean3.append(np.sum(l_train[i,1:]))\n",
    "            loss_rel_mean3.append(l_train[i,0])\n",
    "        if is_lossclassic(names[i]):\n",
    "            name_classic.append(names[i])\n",
    "            loss_total_classic.append(np.sum(l_train[i,1:]))\n",
    "            loss_rel_classic.append(l_train[i,0])\n",
    "\n",
    "    loss_total_loss3 = np.array(loss_total_loss3)\n",
    "    loss_rel_loss3 = np.array(loss_rel_loss3)\n",
    "    _sort_idx = np.argsort(loss_total_loss3)\n",
    "    loss_total_loss3 = loss_total_loss3[_sort_idx]\n",
    "    loss_rel_loss3 = loss_rel_loss3[_sort_idx]\n",
    "    print('Best run sorted by total loss is ', name_loss3[_sort_idx[0]])\n",
    "    print('Best run sorted by relative loss is ', name_loss3[np.argsort(loss_rel_loss3)[0]])\n",
    "\n",
    "\n",
    "    loss_total_mean3 = np.array(loss_total_mean3)\n",
    "    loss_rel_mean3 = np.array(loss_rel_mean3)\n",
    "    _sort_idx = np.argsort(loss_total_mean3)\n",
    "    loss_total_mean3 = loss_total_mean3[_sort_idx]\n",
    "    loss_rel_mean3 = loss_rel_mean3[_sort_idx]\n",
    "    print('Best run sorted by total loss is ', name_mean3[_sort_idx[0]])\n",
    "    print('Best run sorted by relative loss is ', name_mean3[np.argsort(loss_rel_mean3)[0]])\n",
    "\n",
    "    loss_total_classic = np.array(loss_total_classic)\n",
    "    loss_rel_classic = np.array(loss_rel_classic)\n",
    "    _sort_idx = np.argsort(loss_total_classic)\n",
    "    loss_total_classic = loss_total_classic[_sort_idx]\n",
    "    loss_rel_classic = loss_rel_classic[_sort_idx]\n",
    "    print('Best run sorted by total loss is ', name_classic[_sort_idx[0]])\n",
    "    print('Best run sorted by relative loss is ', name_classic[np.argsort(loss_rel_classic)[0]])\n",
    "\n",
    "    summary_loss3 = {\n",
    "        'loss_total': loss_total_loss3,\n",
    "        'loss_rel': loss_rel_loss3\n",
    "    }\n",
    "    summary_mean3 = {\n",
    "        'loss_total': loss_total_mean3,\n",
    "        'loss_rel': loss_rel_mean3\n",
    "    }\n",
    "    summary_classic = {\n",
    "        'loss_total': loss_total_classic,\n",
    "        'loss_rel': loss_rel_classic\n",
    "    }\n",
    "\n",
    "    return summary_loss3, summary_mean3, summary_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_loss3, summary_mean3, summary_classic = read_summary_clean_all(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_example_setup = '224-18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_mean_loss3 = np.mean(summary_loss3['loss_total'])\n",
    "rel_mean_loss3 = np.mean(summary_loss3['loss_rel'])\n",
    "rel_std_loss3 = np.std(summary_loss3['loss_rel'])\n",
    "total_mean_mean3 = np.mean(summary_mean3['loss_total'])\n",
    "rel_mean_mean3 = np.mean(summary_mean3['loss_rel'])\n",
    "rel_std_mean3 = np.std(summary_mean3['loss_rel'])\n",
    "total_mean_classic = np.mean(summary_classic['loss_total'])\n",
    "rel_mean_classic = np.mean(summary_classic['loss_rel'])\n",
    "rel_std_classic = np.std(summary_classic['loss_rel'])\n",
    "\n",
    "print(f'Loss3: {rel_mean_loss3:.4f}+-{rel_std_loss3:.4f}')\n",
    "print(f'Mean3: {rel_mean_mean3:.4f}+-{rel_std_mean3:.4f}')\n",
    "print(f'Classic: {rel_mean_classic:.4f}+-{rel_std_classic:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,sharey=True, figsize=(3,2))\n",
    "\n",
    "ax.scatter(summary_classic['loss_total'], summary_classic['loss_rel'], label='$\\mathcal{L}^c$ training',marker='s', color=my_discrete_cmap(0))\n",
    "ax.scatter(summary_loss3['loss_total'], summary_loss3['loss_rel'], label='$\\mathcal{L}^s$ training',marker='^', color=my_discrete_cmap(1))\n",
    "ax.scatter(summary_mean3['loss_total'], summary_mean3['loss_rel'], label='$\\mathcal{L}^m$ training',marker='.', color=my_discrete_cmap(2))\n",
    "plt.legend()\n",
    "ax.set_xlabel('Total loss')\n",
    "ax.set_ylabel('Rel-l2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,3))\n",
    "violin_parts = ax.violinplot([summary_classic['loss_rel']*100,summary_loss3['loss_rel']*100,summary_mean3['loss_rel']*100],showmeans=True,showextrema=False)\n",
    "ax.set_xticks([1,2,3],['$\\mathcal{L}^c$','$\\mathcal{L}^s$','$\\mathcal{L}^m$'])\n",
    "ax.set_ylabel('$\\epsilon$ (\\%)')\n",
    "for i, pc in enumerate(violin_parts['bodies']):\n",
    "    pc.set_color(my_discrete_cmap(i))\n",
    "    # pc.set_edgecolor(my_discrete_cmap(i))\n",
    "# fig.savefig('./figs/2dkol_clean_compare_lossfn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_t = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot best case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_loss3, datainfo, observed = get_summary_onecase(Path(results_dir,'3-'+clean_example_setup))\n",
    "k_nyquist = (2*np.pi / np.sqrt(2*(datainfo.dx**2))) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_sensor_locations(results_loss3, observed, plt_t, '2dkol_clean_sensor_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_snapshots_clean(data, figname, t1):\n",
    "\n",
    "    ref = data[0]\n",
    "    interp = data[2]\n",
    "    pred = data[3]\n",
    "    fig = plt.figure(figsize=(6,3))\n",
    "    grid_1 = ImageGrid(fig, (0.04,0,0.54,0.3), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_2 = ImageGrid(fig, (0.04,0.31,0.54,0.3), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_3 = ImageGrid(fig, (0.04,0.62,0.54,0.3), (1,3),cbar_mode='single', share_all=True)\n",
    "\n",
    "    grid_r1 = ImageGrid(fig, (0.64,0,0.36,0.3), (1,2), cbar_mode='single', share_all=True)\n",
    "    grid_r2 = ImageGrid(fig, (0.64,0.31,0.36,0.3), (1,2), cbar_mode='single', share_all=True)\n",
    "    grid_r3 = ImageGrid(fig, (0.64,0.62,0.36,0.3), (1,2), cbar_mode='single', share_all=True)\n",
    "\n",
    "    for i, grid in enumerate([grid_3,grid_2,grid_1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(ref[t1,...,i].T)\n",
    "        im_interp = axes[1].imshow(interp[t1,...,i].T)\n",
    "        im_pred = axes[2].imshow(pred[t1,...,i].T)\n",
    "        # vmin = []\n",
    "        # vmax = []\n",
    "        # for im in [im_ref,im_interp,im_pred]:\n",
    "        #     clims = im.get_clim()\n",
    "        #     vmin.append(clims[0])\n",
    "        #     vmax.append(clims[1])\n",
    "        for im in [im_ref,im_interp,im_pred]:\n",
    "            # im.set_clim(min(vmin),max(vmax))\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "\n",
    "    # errpr\n",
    "    errinterp = np.abs(ref[t1,...]-interp[t1,...])\n",
    "    for i, grid in enumerate([grid_r3,grid_r2,grid_r1]):\n",
    "        imerr_interp = grid.axes_all[0].imshow(errinterp[...,i].T)\n",
    "        imerr_pred = grid.axes_all[1].imshow(np.abs(ref[t1,...,i]-pred[t1,...,i]).T)\n",
    "        for im in [imerr_interp, imerr_pred]:\n",
    "            im.set_clim(imerr_interp.get_clim()[0],imerr_interp.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(imerr_interp, ticks=[0.0, round(0.5*errinterp[...,i].max(),1)])\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    fig.text(0,0.15,'$p$')\n",
    "    fig.text(0,0.48,'$u_2$')\n",
    "    fig.text(0,0.81,'$u_1$')\n",
    "    fig.text(0.1,0.97,'Reference')\n",
    "    fig.text(0.24,0.97,'Interpolated')\n",
    "    fig.text(0.39,0.97,'Reconstructed')\n",
    "    fig.text(0.75,0.99,'Absolute error')\n",
    "    fig.text(0.67,0.93,'Interpolated')\n",
    "    fig.text(0.82,0.93,'Reconstructed')\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_snapshots_clean(results_loss3, '2dkol_clean_snapshots'+str(plt_t), plt_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_pred, kbins= get_tke(results_loss3[3], datainfo)\n",
    "spectrum_ref, _ = get_tke(results_loss3[0], datainfo)\n",
    "spectrum_interp, _ = get_tke(results_loss3[2], datainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = [15,39]\n",
    "\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax1 = fig.add_subplot(122, anchor=(0.7,0.0))\n",
    "# fig, (ax0, ax1) = plt.subplots(1,2,figsize=(7,3))\n",
    "ax1.plot(kbins, spectrum_ref, label='Referece', color=grey, alpha=0.5, linewidth=3)\n",
    "ax1.plot(kbins, spectrum_interp, label='Interpolated', color=my_discrete_cmap(0))\n",
    "ax1.plot(kbins, spectrum_pred, label='Reconstructed', color=my_discrete_cmap(1),linestyle='--')\n",
    "ax1.set(yscale='log', xscale='log', xlabel='wavenumber', ylabel='TKE')\n",
    "ax1.set_xlim([1,k_nyquist])\n",
    "ax1.legend()\n",
    "\n",
    "ax01 = fig.add_subplot(321)\n",
    "ax01.plot(results_loss3[0][:,probe[0],probe[0],0],color=grey,alpha=0.5,linewidth=5)\n",
    "ax01.plot(results_loss3[2][:,probe[0],probe[0],0],color=my_discrete_cmap(0))\n",
    "ax01.plot(results_loss3[3][:,probe[0],probe[0],0],color=my_discrete_cmap(1),linestyle='--')\n",
    "\n",
    "ax02 = fig.add_subplot(323)\n",
    "ax02.plot(results_loss3[0][:,probe[0],probe[0],1],color=grey,alpha=0.5,linewidth=5)\n",
    "ax02.plot(results_loss3[2][:,probe[0],probe[0],1],color=my_discrete_cmap(0))\n",
    "ax02.plot(results_loss3[3][:,probe[0],probe[0],1],color=my_discrete_cmap(1),linestyle='--')\n",
    "\n",
    "ax03 = fig.add_subplot(325)\n",
    "ax03.plot(results_loss3[0][:,probe[0],probe[0],2],color=grey,alpha=0.5,linewidth=5)\n",
    "ax03.plot(results_loss3[2][:,probe[0],probe[0],2],color=my_discrete_cmap(0))\n",
    "ax03.plot(results_loss3[3][:,probe[0],probe[0],2],color=my_discrete_cmap(1),linestyle='--')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot edge case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_extra, datainfo_extra, observed_extra = get_summary_onecase('../local_results/2dkol/repeat_clean_minimum/extreme_case_testruns/k2rpb2pi3240628182435') #[clean,noisy,interp,pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(relative_error(results_extra[3],results_extra[0]))\n",
    "print(relative_error(results_extra[2],results_extra[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vorticity\n",
    "vort_train_clean = vorticity(results_extra[0][...,:-1],datainfo_extra)\n",
    "vort_train_pred = vorticity(results_extra[3][...,:-1],datainfo_extra)\n",
    "vort_train_interp = vorticity(results_extra[2][...,:-1],datainfo_extra)\n",
    "# spectrum\n",
    "spectrum_pred_extra, kbins = get_tke(results_extra[3], datainfo_extra)\n",
    "spectrum_ref, _ = get_tke(results_extra[0], datainfo_extra)\n",
    "spectrum_interp_extra, _= get_tke(results_extra[2], datainfo_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_t_step = 1000\n",
    "\n",
    "fig = plt.figure(figsize=(7,2.5))\n",
    "grid = ImageGrid(fig,(0.05,0.1,0.45,0.85),(3,4),share_all=True,cbar_location='right',cbar_mode='single')\n",
    "ax = fig.add_axes((0.68,0.2,0.3,0.7))\n",
    "\n",
    "vmax = np.max(vort_train_clean[0:3*plt_t_step:plt_t_step,...])-1\n",
    "vmin = np.min(vort_train_clean[0:3*plt_t_step:plt_t_step,...])+1\n",
    "axes = grid.axes_all\n",
    "for i in range(3):\n",
    "    im0 = axes[i].imshow(vort_train_clean[plt_t_step*i,:,:].T,vmin=vmin,vmax=vmax)\n",
    "    im1 = axes[i+4].imshow(vort_train_pred[plt_t_step*i,:,:].T,vmin=vmin,vmax=vmax)\n",
    "    cbar = grid.cbar_axes[i].colorbar(im0)\n",
    "    cbar = grid.cbar_axes[i].colorbar(im0,label='Vorticity')\n",
    "    im2 = axes[i+8].imshow(vort_train_interp[plt_t_step*i,:,:].T,vmin=vmin,vmax=vmax)\n",
    "    axes[i+8].set_xlabel(xlabel=f'$t={int(plt_t_step*i*datainfo_extra.dt)}$')\n",
    "imm0 = axes[3].imshow(np.mean(vort_train_clean,axis=0).T,vmin=vmin,vmax=vmax)\n",
    "imm1 = axes[7].imshow(np.mean(vort_train_pred,axis=0).T,vmin=vmin,vmax=vmax)\n",
    "imm2 = axes[11].imshow(np.mean(vort_train_interp,axis=0).T,vmin=vmin,vmax=vmax)\n",
    "axes[11].set_xlabel('Mean')\n",
    "\n",
    "\n",
    "axes[0].set(yticks=[],ylabel='Ref.')\n",
    "axes[0].spy(observed_extra[0,...,-1], color='r', marker='s', markersize=2, alpha=0.6, zorder=2)\n",
    "axes[0].spy(observed_extra[0,...,0], color='k', marker='s', markersize=2, zorder=5)\n",
    "axes[4].set(yticks=[],ylabel='Reconstructed')\n",
    "axes[8].set(yticks=[],ylabel='Interp.')\n",
    "for g in axes:\n",
    "    g.set(xticks=[],xticklabels=[])\n",
    "    g.tick_params(bottom=False,top=False)\n",
    "\n",
    "ax.plot(kbins, spectrum_ref, label='Reference', color=grey, alpha=0.5, linewidth=3)\n",
    "ax.plot(kbins, spectrum_interp_extra, label='Interpolated', color=my_discrete_cmap(0))\n",
    "ax.plot(kbins, spectrum_pred_extra, label='Reconstructed', color=my_discrete_cmap(1),linestyle='--')\n",
    "ax.set(yscale='log', xscale='log', xlabel='wavenumber', ylabel='TKE')\n",
    "ax.set_xlim([1,k_nyquist])\n",
    "ax.legend()\n",
    "\n",
    "plt.savefig('./figs/2dkol_clean_10sensors_overall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make title card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = get_summary_onecase('../local_results/2dkol/repeat_clean_minimum/extreme_case_testruns/k2rpb2pi3240628182435',predict_only=True) #[clean,noisy,interp,pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "example_sensors = [*it.permutations(range(128),2)]\n",
    "np.random.shuffle(example_sensors)\n",
    "example_sensors = np.array(example_sensors[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(1.5,1.5))\n",
    "ax = fig.add_subplot(111)\n",
    "example_pred_masked = np.empty_like(example_pred[1000,:,:,-1])\n",
    "ax.imshow(example_pred[1000,:,:,0],cmap='Greys',alpha=0.4)\n",
    "ax.scatter(example_sensors[:,0],example_sensors[:,1],s=3,c=example_pred[1000,example_sensors[:,0],example_sensors[:,1],0],cmap='jet',marker='s')\n",
    "ax.set(xticks=[],yticks=[])\n",
    "fig.savefig('./figs/title_input',bbox_inches = 'tight', pad_inches = 0)\n",
    "for i in range(3):\n",
    "    fig = plt.figure(figsize=(1.5,1.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(example_pred[1000,:,:,i],cmap='jet',origin='lower')\n",
    "    ax.set(xticks=[],yticks=[])\n",
    "    fig.savefig(f'./figs/title_output{i}', bbox_inches = 'tight', pad_inches = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_snapshots_vorticity(data, datainfo, figname, t1):\n",
    "    # data is (ref, noisy, interp, classic, loss3, mean3)\n",
    "\n",
    "    ref = data[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    \n",
    "    # grids for mean\n",
    "    grid_b1 = ImageGrid(fig, (0.08,0.00,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b2 = ImageGrid(fig, (0.08,0.23,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # grids for snapshots\n",
    "    grid_t1 = ImageGrid(fig, (0.08,0.52,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t2 = ImageGrid(fig, (0.08,0.75,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    data_vort = []\n",
    "    for _data in data:\n",
    "        data_vort.append(\n",
    "            vorticity(_data[...,:2], datainfo)\n",
    "        )\n",
    "    \n",
    "    axes = grid_t2.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        data_vort[0][t1,...].T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            data_vort[j][t1,...].T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_t2.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_t2.axes_all[0].set(xticks=[],yticks=[])\n",
    "    axes = grid_t1.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        ref[t1,...,2].T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            data[j][t1,...,2].T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_t1.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_t1.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    # mean\n",
    "    axes = grid_b2.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        np.mean(data_vort[0],axis=0).T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            np.mean(data_vort[j],axis=0).T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_b2.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_b2.axes_all[0].set(xticks=[],yticks=[])\n",
    "    axes = grid_b1.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        np.mean(ref[...,2],axis=0).T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            np.mean(data[j][...,2],axis=0).T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_b1.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_b1.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    " \n",
    "    \n",
    "    fig.text(0.11,0.98,'Reference')\n",
    "    fig.text(0.28,0.98,'Noisy')\n",
    "    fig.text(0.41,0.98,'Interpolated')\n",
    "    fig.text(0.60,0.98,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.75,0.98,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.9,0.98,'$\\mathcal{L}^m$')\n",
    "    fig.text(0.005,0.20,'Mean',rotation='vertical')\n",
    "    fig.text(0.05,0.1, '$\\overline{p}$')\n",
    "    fig.text(0.05,0.3, '$\\overline{v}$')\n",
    "    fig.text(0.005,0.70,f't={t1*datainfo.dt:.1f}',rotation='vertical')\n",
    "    fig.text(0.05,0.6, '$p$')\n",
    "    fig.text(0.05,0.85, '$v$')\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_snapshots(data, datainfo, figname, t1):\n",
    "    # data is (ref, noisy, interp, classic, loss3, mean3)\n",
    "\n",
    "    ref = data[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    \n",
    "    # grids for mean\n",
    "    grid_b1 = ImageGrid(fig, (0.08,0.00,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b2 = ImageGrid(fig, (0.08,0.15,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b3 = ImageGrid(fig, (0.08,0.30,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # grids for snapshots\n",
    "    grid_t1 = ImageGrid(fig, (0.08,0.50,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t2 = ImageGrid(fig, (0.08,0.65,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t3 = ImageGrid(fig, (0.08,0.80,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # snapshots\n",
    "    for i, grid in enumerate([grid_t3,grid_t2,grid_t1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(ref[t1,...,i].T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(data[j][t1,...,i].T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    # mean\n",
    "    for i, grid in enumerate([grid_b3,grid_b2,grid_b1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(np.mean(ref[...,i],axis=0).T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(np.mean(data[j][...,i],axis=0).T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    fig.text(0.11,0.98,'Reference')\n",
    "    fig.text(0.28,0.98,'Noisy')\n",
    "    fig.text(0.41,0.98,'Interpolated')\n",
    "    fig.text(0.60,0.98,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.75,0.98,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.9,0.98,'$\\mathcal{L}^m$')\n",
    "    fig.text(0.01,0.20,'Mean',rotation='vertical')\n",
    "    fig.text(0.05,0.07, '$\\overline{p}$')\n",
    "    fig.text(0.05,0.22, '$\\overline{u}_2$')\n",
    "    fig.text(0.05,0.37, '$\\overline{u}_1$')\n",
    "    fig.text(0.01,0.70,f't={t1*datainfo.dt:.1f}',rotation='vertical')\n",
    "    fig.text(0.05,0.57, '$p$')\n",
    "    fig.text(0.05,0.72, '$u_2$')\n",
    "    fig.text(0.05,0.87, '$u_1$')\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_noisy_dir = Path('../local_results/2dkol/repeat_noisy')\n",
    "\n",
    "psnr5 = re.compile('snr5')\n",
    "psnr10 = re.compile('snr10')\n",
    "psnr20 = re.compile('snr20')\n",
    "\n",
    "def is_snr5(name):\n",
    "    if re.search(psnr5, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_snr10(name):\n",
    "    if re.search(psnr10, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "def is_snr20(name):\n",
    "    if re.search(psnr20, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "p3n = re.compile('^loss3-')\n",
    "def is_loss3_noisy(name):\n",
    "    if re.search(p3n, name):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary_noisy_all(d, lossfn):\n",
    "\n",
    "    is_lossfn = {'loss3': is_loss3_noisy, 'lossmean': is_lossmean, 'lossclassic': is_lossclassic}[lossfn]\n",
    "\n",
    "    with h5py.File(Path(d,'summary.h5')) as hf:\n",
    "        names = np.array(hf.get('runs_name')).astype('unicode')\n",
    "        l_train = np.array(hf.get('runs_loss_train'))\n",
    "\n",
    "    lrel_20 = []\n",
    "    lp_20 = []\n",
    "    ltotal_20 = []\n",
    "    name_20 = []\n",
    "    lrel_10 = []\n",
    "    lp_10 = []\n",
    "    ltotal_10 = []\n",
    "    name_10 = []\n",
    "    lrel_5 = []\n",
    "    lp_5 = []\n",
    "    ltotal_5 = []\n",
    "    name_5 = []\n",
    "\n",
    "    for i in range(len(names)):\n",
    "        _snr, _lossfn = names[i].split('_')\n",
    "        if is_lossfn(_lossfn):\n",
    "            if is_snr20(_snr):\n",
    "                name_20.append(names[i])\n",
    "                ltotal_20.append(np.sum(l_train[i,1:]))\n",
    "                lrel_20.append(l_train[i,0])\n",
    "                lp_20.append(np.sum(l_train[i,1:3]))\n",
    "            if is_snr10(_snr):\n",
    "                name_10.append(names[i])\n",
    "                ltotal_10.append(np.sum(l_train[i,1:]))\n",
    "                lrel_10.append(l_train[i,0])\n",
    "                lp_10.append(np.sum(l_train[i,1:3]))\n",
    "            if is_snr5(_snr):\n",
    "                name_5.append(names[i])\n",
    "                ltotal_5.append(np.sum(l_train[i,1:]))\n",
    "                lrel_5.append(l_train[i,0])\n",
    "                lp_5.append(np.sum(l_train[i,1:3]))\n",
    "    ltotal = np.array([ltotal_20,ltotal_10,ltotal_5])\n",
    "    lrel = np.array([lrel_20,lrel_10,lrel_5])\n",
    "    lp = np.array([lp_20,lp_10,lp_5])\n",
    "    name = np.array([name_20,name_10,name_5])\n",
    "    \n",
    "    mintotal = np.argmin(ltotal,axis=1)\n",
    "    minrel = np.argmin(lrel,axis=1)\n",
    "\n",
    "    print('Best run sorted by total loss is ', name[[0,1,2],mintotal])\n",
    "    print('Best run sorted by relative loss is ', name[[0,1,2],minrel])\n",
    "\n",
    "    return ltotal, lrel, lp, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_error_vorticity(data, datainfo, figname, t):\n",
    "    ## results (ref, noisy, interp, classic, loss3, mean3)\n",
    "    ref = data[0]\n",
    "    data_vort = []\n",
    "    for _data in data:\n",
    "        data_vort.append(\n",
    "            vorticity(_data[...,:2], datainfo).reshape(_data.shape[:-1]+(1,))\n",
    "        )\n",
    "\n",
    "    fig = plt.figure(figsize=(7.5,3.5))\n",
    "    # grids for mean\n",
    "    grid_b1 = ImageGrid(fig, (0.07,0.00,0.58,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b2 = ImageGrid(fig, (0.07,0.23,0.58,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "\n",
    "    grid_b1e = ImageGrid(fig, (0.71,0.00,0.29,0.22), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_b2e = ImageGrid(fig, (0.71,0.23,0.29,0.22), (1,3),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # grids for snapshots\n",
    "    grid_t1 = ImageGrid(fig, (0.07,0.49,0.58,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t2 = ImageGrid(fig, (0.07,0.72,0.58,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t1e = ImageGrid(fig, (0.71,0.49,0.29,0.22), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_t2e = ImageGrid(fig, (0.71,0.72,0.29,0.22), (1,3),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    ## snapshots ### here -------------------\n",
    "    # vorticity\n",
    "    for _data, grid in zip([data_vort, data],[grid_t2, grid_t1]):\n",
    "        axes = grid.axes_all \n",
    "        im_ref = axes[0].imshow(\n",
    "            _data[0][t,...,-1].T\n",
    "        )\n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(\n",
    "                _data[j][t,...,-1].T\n",
    "            )\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "\n",
    "    # # snapshot error\n",
    "    for _data, grid in zip([data_vort, data],[grid_t2e,grid_t1e]):\n",
    "        axes = grid.axes_all\n",
    "        \n",
    "        lvmin = []\n",
    "        lvmax = []\n",
    "        for j in range(3):\n",
    "            im = axes[j].imshow(np.abs(_data[0][t,...,-1] - _data[j+3][t,...,-1]).T)\n",
    "            lvmin.append(im.get_clim()[0])\n",
    "            lvmax.append(im.get_clim()[1])\n",
    "        for j in range(3):\n",
    "            axes[j].get_images()[0].set_clim(np.min(lvmin), np.max(lvmax))\n",
    "        grid.cbar_axes[0].colorbar(axes[0].get_images()[0])\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "\n",
    "    \n",
    "    # # mean\n",
    "    for _data, grid in zip([data_vort,data],[grid_b2,grid_b1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(np.mean(_data[0][...,-1],axis=0).T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(np.mean(_data[j][...,-1],axis=0).T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    # mean error\n",
    "    for _data, grid in zip([data_vort,data],[grid_b2e,grid_b1e]):\n",
    "        axes = grid.axes_all\n",
    "        \n",
    "        lvmin = []\n",
    "        lvmax = []\n",
    "        for j in range(3):\n",
    "            im = axes[j].imshow(np.abs(np.mean(_data[0][...,-1],axis=0) - np.mean(_data[j+3][...,-1],axis=0)).T)\n",
    "            lvmin.append(im.get_clim()[0])\n",
    "            lvmax.append(im.get_clim()[1])\n",
    "        for j in range(3):\n",
    "            axes[j].get_images()[0].set_clim(np.min(lvmin), np.max(lvmax))\n",
    "        grid.cbar_axes[0].colorbar(axes[0].get_images()[0])\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    fig.text(0.09,0.98,'Reference')\n",
    "    fig.text(0.20,0.98,'Noisy')\n",
    "    fig.text(0.27,0.98,'Interpolated')\n",
    "    fig.text(0.40,0.98,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.49,0.98,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.58,0.98,'$\\mathcal{L}^m$')\n",
    "    fig.text(0.01,0.20,'Mean',rotation='vertical')\n",
    "    fig.text(0.04,0.1, '$\\overline{p}$')\n",
    "    fig.text(0.04,0.31, '$\\overline{v}$')\n",
    "    fig.text(0.01,0.70,f't={t*datainfo.dt:.1f}',rotation='vertical')\n",
    "    fig.text(0.04,0.6, '$p$')\n",
    "    fig.text(0.04,0.83, '$v$')\n",
    "    fig.text(0.81,0.99, 'Absolute error')\n",
    "    fig.text(0.75,0.95,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.84,0.95,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.93,0.95,'$\\mathcal{L}^m$')\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_error(data, datainfo, figname, t):\n",
    "    ## results (ref, noisy, interp, classic, loss3, mean3)\n",
    "    ref = data[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(7.5,5))\n",
    "    # grids for mean\n",
    "    grid_b1 = ImageGrid(fig, (0.07,0.00,0.58,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b2 = ImageGrid(fig, (0.07,0.15,0.58,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b3 = ImageGrid(fig, (0.07,0.30,0.58,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "\n",
    "    grid_b1e = ImageGrid(fig, (0.71,0.00,0.29,0.14), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_b2e = ImageGrid(fig, (0.71,0.15,0.29,0.14), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_b3e = ImageGrid(fig, (0.71,0.30,0.29,0.14), (1,3),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # grids for snapshots\n",
    "    grid_t1 = ImageGrid(fig, (0.07,0.50,0.58,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t2 = ImageGrid(fig, (0.07,0.65,0.58,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t3 = ImageGrid(fig, (0.07,0.80,0.58,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t1e = ImageGrid(fig, (0.71,0.50,0.29,0.14), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_t2e = ImageGrid(fig, (0.71,0.65,0.29,0.14), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_t3e = ImageGrid(fig, (0.71,0.80,0.29,0.14), (1,3),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # snapshots\n",
    "    for i, grid in enumerate([grid_t3,grid_t2,grid_t1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(ref[t,...,i].T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(data[j][t,...,i].T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    # snapshot error\n",
    "    for i, grid in enumerate([grid_t3e,grid_t2e,grid_t1e]):\n",
    "        axes = grid.axes_all\n",
    "        \n",
    "        lvmin = []\n",
    "        lvmax = []\n",
    "        for j in range(3):\n",
    "            im = axes[j].imshow(np.abs(ref[t,...,i] - data[j+3][t,...,i]).T)\n",
    "            lvmin.append(im.get_clim()[0])\n",
    "            lvmax.append(im.get_clim()[1])\n",
    "        for j in range(3):\n",
    "            axes[j].get_images()[0].set_clim(np.min(lvmin), np.max(lvmax))\n",
    "        grid.cbar_axes[0].colorbar(axes[0].get_images()[0])\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "\n",
    "    \n",
    "    # mean\n",
    "    for i, grid in enumerate([grid_b3,grid_b2,grid_b1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(np.mean(ref[...,i],axis=0).T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(np.mean(data[j][...,i],axis=0).T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    # mean error\n",
    "    for i, grid in enumerate([grid_b3e,grid_b2e,grid_b1e]):\n",
    "        axes = grid.axes_all\n",
    "        \n",
    "        lvmin = []\n",
    "        lvmax = []\n",
    "        for j in range(3):\n",
    "            im = axes[j].imshow(np.abs(np.mean(ref[...,i],axis=0) - np.mean(data[j+3][...,i],axis=0)).T)\n",
    "            lvmin.append(im.get_clim()[0])\n",
    "            lvmax.append(im.get_clim()[1])\n",
    "        for j in range(3):\n",
    "            axes[j].get_images()[0].set_clim(np.min(lvmin), np.max(lvmax))\n",
    "        grid.cbar_axes[0].colorbar(axes[0].get_images()[0])\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    fig.text(0.09,0.98,'Reference')\n",
    "    fig.text(0.20,0.98,'Noisy')\n",
    "    fig.text(0.27,0.98,'Interpolated')\n",
    "    fig.text(0.40,0.98,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.49,0.98,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.58,0.98,'$\\mathcal{L}^m$')\n",
    "    fig.text(0.01,0.20,'Mean',rotation='vertical')\n",
    "    fig.text(0.04,0.07, '$\\overline{p}$')\n",
    "    fig.text(0.04,0.22, '$\\overline{u}_2$')\n",
    "    fig.text(0.04,0.37, '$\\overline{u}_1$')\n",
    "    fig.text(0.01,0.70,f't={t*datainfo.dt:.1f}',rotation='vertical')\n",
    "    fig.text(0.04,0.57, '$p$')\n",
    "    fig.text(0.04,0.72, '$u_2$')\n",
    "    fig.text(0.04,0.87, '$u_1$')\n",
    "    fig.text(0.81,0.99, 'Absolute error')\n",
    "    fig.text(0.75,0.96,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.84,0.96,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.93,0.96,'$\\mathcal{L}^m$')\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall comparison all signal to noise ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltn_classic, lreln_classic, lpn_classic, namen_classic = read_summary_noisy_all(results_noisy_dir, 'lossclassic')\n",
    "ltn_loss3, lreln_loss3, lpn_loss3, namen_loss3 = read_summary_noisy_all(results_noisy_dir, 'loss3')\n",
    "ltn_mean, lreln_mean, lpn_mean, namen_mean = read_summary_noisy_all(results_noisy_dir, 'lossmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = '7521-42135'\n",
    "best_noisy_run = [\n",
    "    ['snr20_classic-'+seeds, 'snr10_classic-'+seeds, 'snr5_classic-'+seeds],\n",
    "    ['snr20_loss3-'+seeds, 'snr10_loss3-'+seeds, 'snr5_loss3-'+seeds],\n",
    "    ['snr20_mean3-'+seeds, 'snr10_mean3-'+seeds, 'snr5_mean3-'+seeds]\n",
    "] #[lossfn, snr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = [20,10,5]\n",
    "lp_ref = 1.1317265e-05\n",
    "\n",
    "fig, (axl, axr) = plt.subplots(1,2,figsize=(7,2.5), sharex=True)\n",
    "\n",
    "mean_lreln_classic = np.mean(lreln_classic*100,axis=1)\n",
    "std_lreln_classic = np.std(lreln_classic*100,axis=1)\n",
    "mean_lreln_loss3 = np.mean(lreln_loss3*100,axis=1)\n",
    "std_lreln_loss3 = np.std(lreln_loss3*100,axis=1)\n",
    "mean_lreln_mean = np.mean(lreln_mean*100,axis=1)\n",
    "std_lreln_mean = np.std(lreln_mean*100,axis=1)\n",
    "\n",
    "mean_lpn_classic = np.mean(lpn_classic,axis=1)\n",
    "std_lpn_classic = np.std(lpn_classic,axis=1)\n",
    "mean_lpn_loss3 = np.mean(lpn_loss3,axis=1)\n",
    "std_lpn_loss3 = np.std(lpn_loss3,axis=1)\n",
    "mean_lpn_mean = np.mean(lpn_mean,axis=1)\n",
    "std_lpn_mean = np.std(lpn_mean,axis=1)\n",
    "\n",
    "axl.errorbar(snr,mean_lreln_classic,yerr=std_lreln_classic,label='$\\mathcal{L}^c$ ',marker='x',color=my_discrete_cmap(0),linewidth=2.5)\n",
    "axl.errorbar(snr,mean_lreln_loss3,yerr=std_lreln_loss3,label='$\\mathcal{L}^s$ ',marker='x',color=my_discrete_cmap(1),linewidth=2.5)\n",
    "axl.errorbar(snr,mean_lreln_mean,yerr=std_lreln_mean,label='$\\mathcal{L}^m$ ',marker='x',color=my_discrete_cmap(2),linewidth=2.5)\n",
    "\n",
    "axl.set_ylabel('$\\epsilon (\\%)$')\n",
    "axl.set_xticks([5,10,20])\n",
    "axl.set_xlabel('SNR')\n",
    "\n",
    "axr.errorbar(snr,np.mean(lpn_classic,axis=1),yerr=np.std(lpn_classic,axis=1),marker='x',color=my_discrete_cmap(0),linewidth=2.5)\n",
    "axr.errorbar(snr,np.mean(lpn_loss3,axis=1),yerr=np.std(lpn_loss3,axis=1), marker='x',color=my_discrete_cmap(1),linewidth=2.5)\n",
    "axr.errorbar(snr,np.mean(lpn_mean,axis=1),yerr=np.std(lpn_mean,axis=1),marker='x',color=my_discrete_cmap(2),linewidth=2.5)\n",
    "axr.hlines(lp_ref, xmin=5,xmax=20, colors=['k'], linestyles='dashed',label='reference data')\n",
    "# axr.legend(ncol=1)\n",
    "axr.set_ylabel('$\\mathcal{L}_p$')\n",
    "axr.set_xticks([5,10,20])\n",
    "axr.set_xlabel('SNR')\n",
    "\n",
    "fig.legend(ncol=4,loc='upper center', bbox_to_anchor=(0.5, 1.1))\n",
    "fig.tight_layout()\n",
    "fig.savefig('./figs/2dkol_noisy_compare_lossfn',bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Relative error')\n",
    "print(f'        SNR          20            10               5')\n",
    "print(f'lossclassic  {mean_lreln_classic[0]:.3f}+-{std_lreln_classic[0]:.5f}  {mean_lreln_classic[1]:.3f}+-{std_lreln_classic[1]:.5f}  {mean_lreln_classic[2]:.3f}+-{std_lreln_classic[2]:.5f}')\n",
    "print(f'loss3        {mean_lreln_loss3[0]:.3f}+-{std_lreln_loss3[0]:.5f}  {mean_lreln_loss3[1]:.3f}+-{std_lreln_loss3[1]:.5f}  {mean_lreln_loss3[2]:.3f}+-{std_lreln_loss3[2]:.5f}')\n",
    "print(f'lossmean     {mean_lreln_mean[0]:.3f}+-{std_lreln_mean[0]:.5f}  {mean_lreln_mean[1]:.3f}+-{std_lreln_mean[1]:.5f}  {mean_lreln_mean[2]:.3f}+-{std_lreln_mean[2]:.5f}')\n",
    "\n",
    "print(f'Physics loss')\n",
    "print(f'        SNR          20            10               5')\n",
    "print(f'lossclassic  {mean_lpn_classic[0]:.3f}+-{std_lpn_classic[0]:.5f}  {mean_lpn_classic[1]:.3f}+-{std_lpn_classic[1]:.5f}  {mean_lpn_classic[2]:.3f}+-{std_lpn_classic[2]:.5f}')\n",
    "print(f'loss3        {mean_lpn_loss3[0]:.3f}+-{std_lpn_loss3[0]:.5f}  {mean_lpn_loss3[1]:.3f}+-{std_lpn_loss3[1]:.5f}  {mean_lpn_loss3[2]:.3f}+-{std_lpn_loss3[2]:.5f}')\n",
    "print(f'lossmean     {mean_lpn_mean[0]:.3f}+-{std_lpn_mean[0]:.5f}  {mean_lpn_mean[1]:.3f}+-{std_lpn_mean[1]:.5f}  {mean_lpn_mean[2]:.3f}+-{std_lpn_mean[2]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots of cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_tn = 1931"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results10, datainfo, observed = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[0][1]))\n",
    "_results10_loss3 = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[1][1]), predict_only=True)\n",
    "_results10_mean3 = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[2][1]), predict_only=True)\n",
    "results10 = list(results10)\n",
    "results10.append(_results10_loss3)\n",
    "results10.append(_results10_mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_sensor_locations(results10, observed, plt_tn, '2dkol_noisy_sensor_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_error(results10,datainfo,'2dkol_noisy10_snapshots'+str(plt_tn),plt_tn)\n",
    "make_image_error_vorticity(results10,datainfo,'2dkol_noisy10_snapshots_vorticity'+str(plt_tn),plt_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNR 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results5, datainfo, observed = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[0][2]))\n",
    "_results5_loss3 = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[1][2]), predict_only=True)\n",
    "_results5_mean3 = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[2][2]), predict_only=True)\n",
    "results5 = list(results5)\n",
    "results5.append(_results5_loss3)\n",
    "results5.append(_results5_mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_error(results5,datainfo,'2dkol_noisy5_snapshots'+str(plt_tn),plt_tn)\n",
    "make_image_error_vorticity(results5,datainfo,'2dkol_noisy5_snapshots_vorticity'+str(plt_tn),plt_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_error(results5[-1],results5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNR20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results20, datainfo, observed = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[0][0]))\n",
    "_results20_loss3 = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[1][0]), predict_only=True)\n",
    "_results20_mean3 = get_summary_onecase(Path(results_noisy_dir,best_noisy_run[2][0]), predict_only=True)\n",
    "results20 = list(results20)\n",
    "results20.append(_results20_loss3)\n",
    "results20.append(_results20_mean3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_error(results20,datainfo,'2dkol_noisy20_snapshots'+str(plt_tn),plt_tn)\n",
    "make_image_error_vorticity(results20,datainfo,'2dkol_noisy20_snapshots_vorticity'+str(plt_tn),plt_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## results (ref, noisy, interp, classic, loss3, mean3)\n",
    "\n",
    "spectrum_ref, kbins = get_tke(results20[0]-np.mean(results20[0],axis=0,keepdims=True), datainfo)\n",
    "spectrum_interp20, _ = get_tke(results20[2]-np.mean(results20[2],axis=0,keepdims=True), datainfo)\n",
    "spectrum_lossclassic20, _ = get_tke(results20[3]-np.mean(results20[3],axis=0,keepdims=True), datainfo)\n",
    "spectrum_loss320, _ = get_tke(results20[4]-np.mean(results20[4],axis=0,keepdims=True), datainfo)\n",
    "spectrum_lossmean20, _ = get_tke(results20[5]-np.mean(results20[5],axis=0,keepdims=True), datainfo)\n",
    "\n",
    "spectrum_interp10, _ = get_tke(results10[2]-np.mean(results10[2],axis=0,keepdims=True), datainfo)\n",
    "spectrum_lossclassic10, _ = get_tke(results10[3]-np.mean(results10[3],axis=0,keepdims=True), datainfo)\n",
    "spectrum_loss310, _ = get_tke(results10[4]-np.mean(results10[4],axis=0,keepdims=True), datainfo)\n",
    "spectrum_lossmean10, _ = get_tke(results10[5]-np.mean(results10[5],axis=0,keepdims=True), datainfo)\n",
    "\n",
    "spectrum_interp5, _ = get_tke(results5[2]-np.mean(results5[2],axis=0,keepdims=True), datainfo)\n",
    "spectrum_lossclassic5, _ = get_tke(results5[3]-np.mean(results5[3],axis=0,keepdims=True), datainfo)\n",
    "spectrum_loss35, _ = get_tke(results5[4]-np.mean(results5[4],axis=0,keepdims=True), datainfo)\n",
    "spectrum_lossmean5, _ = get_tke(results5[5]-np.mean(results5[5],axis=0,keepdims=True), datainfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1,axes = plt.subplots(1,3,figsize=(7,3),sharey=True)\n",
    "\n",
    "# snr20\n",
    "\n",
    "line0, = axes[0].loglog(kbins, spectrum_ref, label='Referece', color=grey, alpha=0.5, linewidth=3)\n",
    "line1, = axes[0].loglog(kbins, spectrum_interp20, label='Interpolated', color='k',linestyle='--')\n",
    "line2, = axes[0].loglog(kbins, spectrum_lossclassic20, label='$\\mathcal{L}^c$', color=my_discrete_cmap(0))\n",
    "line3, = axes[0].loglog(kbins, spectrum_loss320, label='$\\mathcal{L}^s$', color=my_discrete_cmap(1))\n",
    "line4, = axes[0].loglog(kbins, spectrum_lossmean20, label='$\\mathcal{L}^m$', color=my_discrete_cmap(2))\n",
    "axes[0].set(xlabel='wavenumber', ylabel='TKE')\n",
    "axes[0].set_xlim([1,k_nyquist])\n",
    "axes[0].set_title('SNR20')\n",
    "axes[0].grid(axis='x',which='minor')\n",
    "\n",
    "axes[1].loglog(kbins, spectrum_ref, color=grey, alpha=0.5, linewidth=3)\n",
    "axes[1].loglog(kbins, spectrum_interp10, color='k',linestyle='--')\n",
    "axes[1].loglog(kbins, spectrum_lossclassic10, color=my_discrete_cmap(0))\n",
    "axes[1].loglog(kbins, spectrum_loss310, color=my_discrete_cmap(1))\n",
    "axes[1].loglog(kbins, spectrum_lossmean10, color=my_discrete_cmap(2))\n",
    "axes[1].set(xlabel='wavenumber')\n",
    "axes[1].set_xlim([1,k_nyquist])\n",
    "axes[1].set_title('SNR10')\n",
    "axes[1].grid(axis='x',which='minor')\n",
    "\n",
    "\n",
    "axes[2].loglog(kbins, spectrum_ref, color=grey, alpha=0.5, linewidth=3)\n",
    "axes[2].loglog(kbins, spectrum_interp5, color='k',linestyle='--')\n",
    "axes[2].loglog(kbins, spectrum_lossclassic5, color=my_discrete_cmap(0))\n",
    "axes[2].loglog(kbins, spectrum_loss35, color=my_discrete_cmap(1))\n",
    "axes[2].loglog(kbins, spectrum_lossmean5, color=my_discrete_cmap(2))\n",
    "axes[2].set(xlabel='wavenumber')\n",
    "axes[2].set_xlim([1,k_nyquist])\n",
    "axes[2].set_title('SNR5')\n",
    "axes[2].grid(axis='x',which='minor')\n",
    "\n",
    "fig1.legend(handles = [line0, line1, line2, line3, line4], ncol=5,loc='upper center', bbox_to_anchor=(0.5, 1.11))\n",
    "fig1.savefig('./figs/2dkol_noisy_tke',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probex = 45\n",
    "probey = 94\n",
    "\n",
    "fig,axes = plt.subplots(3,1, figsize=(7,3.5), sharey=True, sharex=True)\n",
    "realt = np.arange(results20[0].shape[0])*datainfo.dt\n",
    "snr = [20,10,5]\n",
    "\n",
    "for i, _data in enumerate([results20,results10,results5]):\n",
    "    axes[i].plot(realt,_data[1][:,probex,probey,0], label='Noisy',color=grey,alpha=0.5)\n",
    "    axes[i].plot(realt,_data[2][:,probex,probey,0], label='Interpolated', color='k',alpha=0.5)\n",
    "    axes[i].plot(realt,_data[4][:,probex,probey,0], label='$\\mathcal{L}^s$', color=my_discrete_cmap(1), linewidth=1, linestyle='--')\n",
    "    axes[i].plot(realt,_data[0][:,probex,probey,0], label='Referece', color='r')\n",
    "    axes[i].plot(realt,_data[5][:,probex,probey,0], label='$\\mathcal{L}^m$', color=my_discrete_cmap(2), linewidth=1.5)\n",
    "    axes[i].plot(realt,_data[3][:,probex,probey,0], label='$\\mathcal{L}^c$', color=my_discrete_cmap(0), linewidth=1.5, linestyle='-.')\n",
    "    axes[i].set_title(f'SNR {snr[i]}')\n",
    "    axes[i].set_ylabel('$u_1$')\n",
    "\n",
    "axes[2].set_xlabel('time (s)')\n",
    "axes[2].set_xlim([0,100])\n",
    "handles,labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles=handles, ncol=6,loc='upper center', bbox_to_anchor=(0.5, 1.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very sparse sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_very_sparse = Path(results_noisy_dir,'min_observations_snr10')\n",
    "very_sparse_runs = ['loss1_24inn_24sensors-7521-42135_1','loss3_24inn_24sensors-7521-42135_1/restart','mean3_24inn_24sensors-7521-42135_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sparse, datainfo, observed_sparse = get_summary_onecase(Path(dir_very_sparse,very_sparse_runs[0]))\n",
    "_results_sparse_loss3 = get_summary_onecase(Path(dir_very_sparse,very_sparse_runs[1]), predict_only=True)\n",
    "_results_sparse_mean3 = get_summary_onecase(Path(dir_very_sparse,very_sparse_runs[2]), predict_only=True)\n",
    "results_sparse = list(results_sparse)\n",
    "results_sparse.append(_results_sparse_loss3)\n",
    "results_sparse.append(_results_sparse_mean3)\n",
    "# results_sparse is (ref, noisy, interp, classic, loss3, mean3)\n",
    "for i in range(2,6):\n",
    "    print(relative_error(results_sparse[i],results_sparse[0])*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_sensor_locations(results_sparse, observed_sparse, plt_tn, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = kolsol_forcing_term(4, 128, 2)\n",
    "print(momentum_loss(results_sparse[-3], datainfo, forcing=f) + divergence(results_sparse[-3][...,:-1], datainfo))\n",
    "print(momentum_loss(results_sparse[-2], datainfo, forcing=f) + divergence(results_sparse[-2][...,:-1], datainfo))\n",
    "print(momentum_loss(results_sparse[-1], datainfo, forcing=f) + divergence(results_sparse[-1][...,:-1], datainfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_error(results_sparse,datainfo,None,plt_tn)\n",
    "make_image_error_vorticity(results_sparse,datainfo,None,plt_tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrum_ref, kbins = get_tke(results_sparse[0]-np.mean(results_sparse[0],axis=0,keepdims=True), datainfo)\n",
    "spectrum_sparse_interp, _ = get_tke(results_sparse[2]-np.mean(results_sparse[2],axis=0,keepdims=True), datainfo)\n",
    "spectrum_sparse_classic, _ = get_tke(results_sparse[3]-np.mean(results_sparse[3],axis=0,keepdims=True), datainfo)\n",
    "spectrum_sparse_loss3, _ = get_tke(results_sparse[4]-np.mean(results_sparse[4],axis=0,keepdims=True), datainfo)\n",
    "spectrum_sparse_lossmean, _ = get_tke(results_sparse[5]-np.mean(results_sparse[5],axis=0,keepdims=True), datainfo)\n",
    "k_nyquist = (2*np.pi / np.sqrt(2*(datainfo.dx**2))) / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7.5,3)) \n",
    "\n",
    "grid1 = ImageGrid(fig,(0.07,0.05,0.60,0.4),(1,5),share_all=True,cbar_location='right',cbar_mode='single')\n",
    "grid2 = ImageGrid(fig,(0.07,0.5,0.60,0.35),(1,5),share_all=True,cbar_location='right',cbar_mode='single')\n",
    "ax1 = fig.add_axes((0.78,0.2,0.21,0.5))\n",
    "\n",
    "im_ref1 = grid1.axes_all[0].imshow(np.mean(results_sparse[0][...,0],axis=0).T)\n",
    "im_ref2 = grid2.axes_all[0].imshow(results_sparse[0][plt_tn,...,0].T)\n",
    "grid1.axes_all[0].spy(observed_sparse[0,...,-1], color='r', marker='s', markersize=1, alpha=0.6, zorder=2)\n",
    "grid1.axes_all[0].spy(observed_sparse[0,...,0], color='k', marker='s', markersize=1, zorder=5)\n",
    "grid2.axes_all[0].spy(observed_sparse[0,...,-1], color='r', marker='s', markersize=1, alpha=0.6, zorder=2)\n",
    "grid2.axes_all[0].spy(observed_sparse[0,...,0], color='k', marker='s', markersize=1, zorder=5)\n",
    "vmin1,vmax1 = im_ref1.get_clim()\n",
    "vmin2,vmax2 = im_ref2.get_clim()\n",
    "for _i, (g1,g2) in enumerate(zip(grid1.axes_all[1:], grid2.axes_all[1:])):\n",
    "    i = _i+2\n",
    "    g1.imshow(np.mean(results_sparse[i][...,0],axis=0).T,vmin=vmin1,vmax=vmax1)\n",
    "    g2.imshow(results_sparse[i][plt_tn,...,0].T,vmin=vmin2,vmax=vmax2)\n",
    "grid1.cbar_axes[0].colorbar(im_ref1)\n",
    "grid1.axes_all[0].set(xticks=[],yticks=[])\n",
    "grid2.cbar_axes[0].colorbar(im_ref2)\n",
    "grid2.axes_all[0].set(xticks=[],yticks=[])\n",
    "\n",
    "\n",
    "l1,  = ax1.loglog(kbins, spectrum_ref, color=grey, alpha=0.5, linewidth=3, label='Ref.')\n",
    "l2, = ax1.loglog(kbins, spectrum_sparse_interp, color='k',linestyle='--', label='Interp.')\n",
    "l3, = ax1.loglog(kbins, spectrum_sparse_classic, color=my_discrete_cmap(0), label='$\\mathcal{L}^c$')\n",
    "l4, = ax1.loglog(kbins, spectrum_sparse_loss3, color=my_discrete_cmap(1), label='$\\mathcal{L}^s$')\n",
    "l5, = ax1.loglog(kbins, spectrum_sparse_lossmean, color=my_discrete_cmap(2), label='$\\mathcal{L}^m$')\n",
    "ax1.set_xlim([1,k_nyquist])\n",
    "ax1.set_yticks([10e-6,10e7])\n",
    "ax1.set_ylabel('TKE', labelpad=-9)\n",
    "ax1.set_xlabel('wavenumber')\n",
    "ax1.grid(axis='x',which='minor')\n",
    "ax1.legend(handles=[l3,l4,l5,l1,l2],ncol=2,loc='upper center', bbox_to_anchor=(0.45, 1.5),fontsize='small')\n",
    "\n",
    "\n",
    "fig.text(0.09,0.9,'Reference')\n",
    "fig.text(0.20,0.9,'Interpolated')\n",
    "fig.text(0.36,0.9,'$\\mathcal{L}^c$')\n",
    "fig.text(0.48,0.9,'$\\mathcal{L}^s$')\n",
    "fig.text(0.60,0.9,'$\\mathcal{L}^m$')\n",
    "fig.text(0.01,0.20,'Mean',rotation='vertical')\n",
    "fig.text(0.04,0.25, '$\\overline{u}_1$')\n",
    "fig.text(0.01,0.6,f't={plt_tn*datainfo.dt:.1f}',rotation='vertical')\n",
    "fig.text(0.04,0.68, '$u_1$')\n",
    "\n",
    "fig.savefig('./figs/2dkol_noisy_sparse_overall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ZOOM IN ON THE LOW WAVENUMBER\n",
    "# fig,ax1 = plt.subplots(1,1,figsize=(4,3))\n",
    "# l1,  = ax1.loglog(kbins, spectrum_ref, color=grey, alpha=0.5, linewidth=3, label='Ref.')\n",
    "# l2, = ax1.loglog(kbins, spectrum_sparse_interp, color='k',linestyle='--', label='Interp.')\n",
    "# l3, = ax1.loglog(kbins, spectrum_sparse_classic, color=my_discrete_cmap(0), label='$\\mathcal{L}^c$')\n",
    "# l4, = ax1.loglog(kbins, spectrum_sparse_loss3, color=my_discrete_cmap(1), label='$\\mathcal{L}^s$')\n",
    "# l5, = ax1.loglog(kbins, spectrum_sparse_lossmean, color=my_discrete_cmap(2), label='$\\mathcal{L}^m$')\n",
    "# ax1.set_xlim([1,15])\n",
    "# ax1.set_ylim([11, 10e8])\n",
    "# ax1.set_yticks([11,10e7])\n",
    "# ax1.set_ylabel('TKE', labelpad=-9)\n",
    "# ax1.set_xlabel('wavenumber')\n",
    "# ax1.grid(axis='x',which='minor')\n",
    "# ax1.legend(handles=[l3,l4,l5,l1,l2],ncol=2,loc='upper center', bbox_to_anchor=(0.45, 1.5),fontsize='small')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, sharex=True, figsize=(7.5,2.3))\n",
    "_labels = ['$\\mathcal{L}^c$', '$\\mathcal{L}^s$', '$\\mathcal{L}^m$']\n",
    "\n",
    "\n",
    "for i in [0,1,2]:\n",
    "    if i == 1:\n",
    "        with h5py.File(Path(dir_very_sparse,very_sparse_runs[i],'../results.h5')) as hf:\n",
    "            loss_train = np.array(hf.get(\"loss_train\"))\n",
    "            loss_div = np.array(hf.get(\"loss_div\"))\n",
    "            loss_momentum = np.array(hf.get(\"loss_momentum\"))\n",
    "            loss_sensors = np.array(hf.get(\"loss_sensors\"))\n",
    "        with h5py.File(Path(dir_very_sparse,very_sparse_runs[i],'results.h5')) as hf:\n",
    "            loss_train = np.concatenate((loss_train,np.array(hf.get(\"loss_train\"))))\n",
    "            loss_div = np.concatenate((loss_div,np.array(hf.get(\"loss_div\"))))\n",
    "            loss_momentum = np.concatenate((loss_momentum,np.array(hf.get(\"loss_momentum\"))))\n",
    "            loss_sensors = np.concatenate((loss_sensors,np.array(hf.get(\"loss_sensors\"))))\n",
    "        epochs = np.arange(0,len(loss_train),50)\n",
    "    else:\n",
    "        with h5py.File(Path(dir_very_sparse,very_sparse_runs[i],'results.h5')) as hf:\n",
    "            loss_train = np.array(hf.get(\"loss_train\"))\n",
    "            loss_div = np.array(hf.get(\"loss_div\"))\n",
    "            loss_momentum = np.array(hf.get(\"loss_momentum\"))\n",
    "            loss_sensors = np.array(hf.get(\"loss_sensors\"))\n",
    "        epochs = np.arange(0,len(loss_train),50)\n",
    "    axes[0].semilogy(epochs, loss_train[::50], color=my_discrete_cmap(i), label=_labels[i])\n",
    "    axes[1].semilogy(epochs, (loss_div+loss_momentum)[::50], color=my_discrete_cmap(i))\n",
    "    axes[1].semilogy(epochs, loss_sensors[::50], color=my_discrete_cmap(i), linestyle=':')\n",
    "    # axes[2].semilogy(epochs, (loss_div+loss_momentum+loss_sensors)[::50], color=my_discrete_cmap(i))\n",
    "    print('physics loss at epoch 5000 and 18000: ',loss_momentum[[5000, 18000]]+loss_div[[5000, 18000]],'data loss at epoch 5000 and 18000: ', loss_sensors[[5000, 18000]])\n",
    "fig.legend(ncols=3, loc='upper center', bbox_to_anchor=(0.5,1.03))\n",
    "axes[0].set(xlim=[0,30000], xlabel='Epochs', ylabel='Loss')\n",
    "axes[1].set(xlim=[0,30000], xlabel='Epochs', ylabel='$\\mathcal{L}_p$ (---), $\\mathcal{L}_o$ ($\\cdot \\! \\, \\cdot \\! \\, \\cdot$)') #'Physics (-) /\\n sensor (..) loss'\n",
    "# axes[2].set(xlabel='Epochs', ylabel='$\\mathcal{L}_p + \\mathcal{L}_o$') # 'Total unweighted loss'\n",
    "fig.subplots_adjust(left=0.08,wspace=0.3,right=0.97,bottom=0.2,top=0.85)\n",
    "\n",
    "fig.savefig('figs/2dkol_noisy_sparse_learning_curve')\n",
    "# fig.savefig('learning_curve.png',bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
