{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import h5py\n",
    "import jax\n",
    "import yaml\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../flowrec/utils/a4.mplstyle')\n",
    "\n",
    "from ml_collections import config_dict\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import RBFInterpolator, griddata\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "import flowrec.data as data_utils\n",
    "import flowrec.physics_and_derivatives as derivatives\n",
    "import flowrec.training_and_states as state_utils\n",
    "from flowrec import losses\n",
    "from flowrec.utils import simulation, my_discrete_cmap\n",
    "from flowrec.utils.py_helper import slice_from_tuple\n",
    "from flowrec.utils.system import set_gpu\n",
    "set_gpu(1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = Path('../local_results/2dtriangle/repeat_clean')\n",
    "dt = 0.125 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(u,pb,case_observe,datacfg):\n",
    "\n",
    "    take_observation, insert_observation = case_observe(datacfg, example_pred_snapshot=u[0,...],example_pin_snapshot=pb[0,...])\n",
    "    observed = take_observation(u)\n",
    "    temp_observed = np.empty_like(u)\n",
    "    temp_observed.fill(np.nan) #this is noisy\n",
    "    temp_observed = insert_observation(jnp.asarray(temp_observed),jnp.asarray(observed)) # observed_test is noisy if\n",
    "\n",
    "    # get sensor coordinates\n",
    "    sensors_empty = np.empty_like(u[[0],...])\n",
    "    sensors_empty.fill(np.nan)\n",
    "\n",
    "    grid_x,grid_y = np.mgrid[0:u[...,0].shape[1], 0:u[...,0].shape[2]]\n",
    "\n",
    "    gridx1 = np.repeat(grid_x[None,:,:,None],3,axis=3)\n",
    "    gridy1 = np.repeat(grid_y[None,:,:,None],3,axis=3)\n",
    "\n",
    "    idx_x = take_observation(gridx1)\n",
    "    idx_y = take_observation(gridy1)\n",
    "\n",
    "    idx_x = insert_observation(jnp.asarray(sensors_empty),jnp.asarray(idx_x))[0,...]\n",
    "    sensors_loc_x = []\n",
    "    for i in range(idx_x.shape[-1]):\n",
    "        sensors_loc_x.append(idx_x[...,i][~np.isnan(idx_x[...,i])])\n",
    "\n",
    "    idx_y = insert_observation(jnp.asarray(sensors_empty),jnp.asarray(idx_y))[0,...]\n",
    "    sensors_loc_y = []\n",
    "    for i in range(idx_y.shape[-1]):\n",
    "        sensors_loc_y.append(idx_y[...,i][~np.isnan(idx_y[...,i])])\n",
    "\n",
    "\n",
    "    compare_interp = list([])\n",
    "    nt = u.shape[0]\n",
    "    _locs = np.stack((grid_x.flatten(),grid_y.flatten()),axis=-1)\n",
    "\n",
    "    for i in range(3):\n",
    "        sensors_loc = np.stack((sensors_loc_x[i].flatten(),sensors_loc_y[i].flatten()),axis=-1)\n",
    "        for j in range(nt):\n",
    "            temp_measurement = temp_observed[j,...,i][~np.isnan(temp_observed[j,...,i])]\n",
    "            # print(sensors_loc.shape, temp_measurement.shape)\n",
    "            rbf = RBFInterpolator(sensors_loc,temp_measurement.flatten(),kernel='thin_plate_spline')\n",
    "            _interp = rbf(_locs).reshape(grid_x.shape)\n",
    "            # _interp = griddata(sensors_loc,temp_measurement,(grid_x,grid_y),'cubic',)\n",
    "            compare_interp.append(_interp)\n",
    "    compare_interp = np.array(compare_interp)\n",
    "    compare_interp = np.stack((compare_interp[:nt,...],compare_interp[nt:2*nt,...],compare_interp[2*nt:3*nt,...]),axis=-1)\n",
    "\n",
    "    return compare_interp, temp_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(results_dir: Path):\n",
    "\n",
    "    with h5py.File(Path(results_dir,'summary.h5')) as hf:\n",
    "        loss_train = np.array(hf.get('runs_loss_train'))\n",
    "        loss_val = np.array(hf.get('runs_loss_val'))\n",
    "    return loss_train, loss_val #[rel_l2, div, momentum, sensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_case_predictions(results_dir):\n",
    "    with open(Path(results_dir,'config.yml'),'r') as f:\n",
    "        cfg = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "    cfg.data_config.update({'data_dir':'.'+cfg.data_config.data_dir})\n",
    "    x_base = 132\n",
    "    triangle_base_coords = [49,80]\n",
    "    (ux,uy,pp) = simulation.read_data_2dtriangle(cfg.data_config.data_dir,x_base)\n",
    "    x = np.stack([ux,uy,pp],axis=0)\n",
    "    # remove parts where uz is not zero\n",
    "    s = slice_from_tuple(cfg.data_config.slice_to_keep)\n",
    "    x = x[s]\n",
    "\n",
    "    # information about the grid\n",
    "    datainfo = data_utils.DataMetadata(\n",
    "        re = cfg.data_config.re,\n",
    "        discretisation=[cfg.data_config.dt,cfg.data_config.dx,cfg.data_config.dy],\n",
    "        axis_index=[0,1,2],\n",
    "        problem_2d=True\n",
    "    ).to_named_tuple()\n",
    "\n",
    "    rng = np.random.default_rng(cfg.data_config.randseed)\n",
    "    if cfg.data_config.snr:\n",
    "        [x_train,x_val,x_test], _ = data_utils.data_partition(x,1,cfg.data_config.train_test_split,REMOVE_MEAN=cfg.data_config.remove_mean,randseed=cfg.data_config.randseed,SHUFFLE=cfg.data_config.shuffle) # Do not shuffle, do not remove mean for training with physics informed loss\n",
    "        [ux_train,uy_train,pp_train] = np.squeeze(np.split(x_train,3,axis=0))\n",
    "        [ux_val,uy_val,pp_val] = np.squeeze(np.split(x_val,3,axis=0))\n",
    "        [ux_test,uy_test,pp_test] = np.squeeze(np.split(x_test,3,axis=0))\n",
    "        u_train = np.stack((ux_train,uy_train,pp_train),axis=-1)\n",
    "        u_val = np.stack((ux_val,uy_val,pp_val),axis=-1)\n",
    "        u_test = np.stack((ux_test,uy_test,pp_test),axis=-1)\n",
    "\n",
    "        \n",
    "        std_data = np.std(x,axis=(1,2,3),ddof=1)\n",
    "        std_n = data_utils.get_whitenoise_std(cfg.data_config.snr,std_data)\n",
    "        noise_ux = rng.normal(scale=std_n[0],size=x[0,...].shape)\n",
    "        noise_uy = rng.normal(scale=std_n[1],size=x[1,...].shape)\n",
    "        noise_pp = rng.normal(scale=std_n[2],size=x[2,...].shape)\n",
    "        noise = np.stack([noise_ux,noise_uy,noise_pp],axis=0)\n",
    "        x = x + noise\n",
    "\n",
    "\n",
    "    [x_train_n,x_val_n,x_test_n], _ = data_utils.data_partition(\n",
    "        x,\n",
    "        1,\n",
    "        cfg.data_config.train_test_split,\n",
    "        REMOVE_MEAN=cfg.data_config.remove_mean,\n",
    "        randseed=cfg.data_config.randseed,\n",
    "        SHUFFLE=cfg.data_config.shuffle\n",
    "    ) # Do not shuffle, do not remove mean for training with physics informed loss\n",
    "    [ux_train_n,uy_train_n,pp_train_n] = np.squeeze(np.split(x_train_n,3,axis=0))\n",
    "    [ux_val_n,uy_val_n,pp_val_n] = np.squeeze(np.split(x_val_n,3,axis=0))\n",
    "    [ux_test_n,uy_test_n,pp_test_n] = np.squeeze(np.split(x_test_n,3,axis=0))\n",
    "    u_train_n = np.stack((ux_train_n,uy_train_n,pp_train_n),axis=-1)\n",
    "    u_val_n = np.stack((ux_val_n,uy_val_n,pp_val_n),axis=-1)\n",
    "    u_test_n = np.stack((ux_test_n,uy_test_n,pp_test_n),axis=-1)\n",
    "\n",
    "    pb_train = simulation.take_measurement_base(pp_train_n,ly=triangle_base_coords,centrex=0).reshape((cfg.data_config.train_test_split[0],-1))\n",
    "    pb_val = simulation.take_measurement_base(pp_val_n,ly=triangle_base_coords,centrex=0).reshape((cfg.data_config.train_test_split[1],-1))\n",
    "    pb_test = simulation.take_measurement_base(pp_test_n,ly=triangle_base_coords,centrex=0).reshape((cfg.data_config.train_test_split[2],-1))\n",
    "\n",
    "    take_observation, insert_observation = cfg.case.observe(cfg.data_config, example_pred_snapshot=u_train_n[0,...],example_pin_snapshot=pb_train[0,...])\n",
    "    observed_train, train_minmax = take_observation(u_train_n,init=True)\n",
    "    observed_val, val_minmax = take_observation(u_val_n,init=True)\n",
    "    observed_test, test_minmax = take_observation(u_test_n,init=True)\n",
    "    \n",
    "    state = state_utils.restore_trainingstate(results_dir,'state')\n",
    "    _, make_model = cfg.case.select_model(datacfg=cfg.data_config, mdlcfg=cfg.model_config, traincfg=cfg.train_config)\n",
    "    mdl = make_model(cfg.model_config)\n",
    "\n",
    "    if cfg.data_config.normalise:\n",
    "        [pb_train, pb_val, pb_test], _ = data_utils.normalise(pb_train, pb_val, pb_test, range=[train_minmax[-1],val_minmax[-1],test_minmax[-1]])\n",
    "\n",
    "    rng = jax.random.PRNGKey(10)\n",
    "\n",
    "    pb_train_batch = np.array_split(pb_train,2,0)\n",
    "    pred_train = []\n",
    "    for inn in pb_train_batch:\n",
    "        pred_train.append(mdl.apply(state.params,rng,inn,TRAINING=False))\n",
    "    pred_train = np.concatenate(pred_train)\n",
    "    pred_test = mdl.apply(state.params,rng,pb_test,TRAINING=False)\n",
    "    if cfg.data_config.normalise:\n",
    "        pred_train = data_utils.unnormalise_group(pred_train, train_minmax, axis_data=-1, axis_range=0)\n",
    "        pred_test = data_utils.unnormalise_group(pred_test, test_minmax, axis_data=-1, axis_range=0)\n",
    "\n",
    "    u_interp, observed = interpolate(u_train_n, pb_train, cfg.case.observe, cfg.data_config)\n",
    "\n",
    "    return (u_train_n, u_interp, pred_train), datainfo, observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, loss_val = get_summary(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For the training set:   \")\n",
    "print(f\"The mean of rel_l2 error: {np.mean(loss_train[:,0])*100:f}, physics loss: {np.mean(np.sum(loss_train[:,1:3],axis=1)):f} and sensor loss: {np.mean(loss_train[:,-1])}\")\n",
    "print(f\"The std of rel_l2 error: {np.std(loss_train[:,0]*100):f}, physics loss: {np.std(np.sum(loss_train[:,1:3],axis=1)):f} and sensor loss: {np.std(loss_train[:,-1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = Path(results_dir,'clean-45')\n",
    "sys.exit() if not best_run.exists() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, datainfo, observed = get_single_case_predictions(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantaneous and mean flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_snapshots(data, figname, t1):\n",
    "\n",
    "    ref = data[0]\n",
    "    interp = data[1]\n",
    "    pred = data[2]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,3))\n",
    "    grid_1 = ImageGrid(fig, (0.04,0,0.54,0.3), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_2 = ImageGrid(fig, (0.04,0.33,0.54,0.3), (1,3),cbar_mode='single', share_all=True)\n",
    "    grid_3 = ImageGrid(fig, (0.04,0.66,0.54,0.3), (1,3),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    grid_r1 = ImageGrid(fig, (0.64,0,0.36,0.3), (1,2), cbar_mode='single', share_all=True)\n",
    "    grid_r2 = ImageGrid(fig, (0.64,0.33,0.36,0.3), (1,2), cbar_mode='single', share_all=True)\n",
    "    grid_r3 = ImageGrid(fig, (0.64,0.66,0.36,0.3), (1,2), cbar_mode='single', share_all=True)\n",
    "\n",
    "    for i, grid in enumerate([grid_3,grid_2,grid_1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(ref[t1,...,i].T)\n",
    "        im_interp = axes[1].imshow(interp[t1,...,i].T)\n",
    "        im_pred = axes[2].imshow(pred[t1,...,i].T)\n",
    "        # vmin = []\n",
    "        # vmax = []\n",
    "        # for im in [im_ref,im_interp,im_pred]:\n",
    "        #     clims = im.get_clim()\n",
    "        #     vmin.append(clims[0])\n",
    "        #     vmax.append(clims[1])\n",
    "        for im in [im_ref,im_interp,im_pred]:\n",
    "            # im.set_clim(min(vmin),max(vmax))\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    for i, grid in enumerate([grid_r3,grid_r2,grid_r1]):\n",
    "        imerr_interp = grid.axes_all[0].imshow(np.abs(ref[t1,...,i]-interp[t1,...,i]).T)\n",
    "        imerr_pred = grid.axes_all[1].imshow(np.abs(ref[t1,...,i]-pred[t1,...,i]).T)\n",
    "        for im in [imerr_interp, imerr_pred]:\n",
    "            im.set_clim(imerr_interp.get_clim()[0],imerr_interp.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(imerr_interp)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    fig.text(0,0.15,'$p$')\n",
    "    fig.text(0,0.48,'$u_2$')\n",
    "    fig.text(0,0.81,'$u_1$')\n",
    "    fig.text(0.09,0.97,'Reference')\n",
    "    fig.text(0.25,0.97,'Interpolated')\n",
    "    fig.text(0.44,0.97,'Reconstructed')\n",
    "    fig.text(0.75,0.99,'Absolute error')\n",
    "    fig.text(0.66,0.93,'Interpolated')\n",
    "    fig.text(0.82,0.93,'Reconstructed')\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = 100\n",
    "# make_image_snapshots(results, '2dtriangle_clean_snapshots'+str(time), 100) \n",
    "make_image_snapshots(results,None, 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_pdf(data,figname):\n",
    "\n",
    "    ## interpolation\n",
    "    interp_test_nonan = []\n",
    "    for i in range(3):\n",
    "        mask = ~np.isnan(data[1][0,...,i])\n",
    "        interp_test_nonan.append(data[1][...,i][:,mask])\n",
    "\n",
    "    ## plot\n",
    "    fig,axes = plt.subplots(1,3,figsize=(7,2.5))\n",
    "    for i,var in zip(range(3),['$u_1^\\prime$','$u_2^\\prime$','$p^\\prime$']):\n",
    "        # true\n",
    "        counts_true,bins_true = np.histogram(data[0][...,i].flatten()-np.mean(data[0][...,i].flatten()), density=True, bins='auto')\n",
    "        axes[i].stairs(counts_true,bins_true,label='True',linewidth=3, color='#808080',alpha=0.5)\n",
    "        # interpolation\n",
    "        counts,bins = np.histogram(interp_test_nonan[i].flatten()-np.mean(interp_test_nonan[i].flatten()), density=True, bins='auto')\n",
    "        axes[i].stairs(counts,bins,label='Interp.',color='k',linestyle='--')\n",
    "        \n",
    "        # prediction\n",
    "        counts,bins = np.histogram(data[2][...,i].flatten()-np.mean(data[2][...,i].flatten()), density=True, bins='auto')\n",
    "        axes[i].stairs(counts,bins,label='Reconstructed',color=my_discrete_cmap(0))\n",
    "        axes[i].set(xlabel=var)\n",
    "    axes[0].set_ylabel('Probability density')\n",
    "\n",
    "    handlers = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(*handlers,loc='upper center', bbox_to_anchor=(0.5, 1.15),ncols=5)\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_pdf(results,'2dtriangle_clean_pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['$u_1^\\prime$','$u_2^\\prime$','$p^\\prime$']\n",
    "\n",
    "def make_image_freq(data, figname):\n",
    "\n",
    "    n = np.prod(data[0].shape[1:-1])\n",
    "\n",
    "    fftfreq = np.fft.fftfreq(data[0].shape[0],d = dt)\n",
    "    midpoint = int(len(fftfreq)/2)\n",
    "    \n",
    "    # data\n",
    "    f_data = np.einsum('t x y u -> t u', np.abs(np.fft.fft(data[0]-np.mean(data[0],axis=0),axis=0)))\n",
    "\n",
    "    # interpolated\n",
    "    interp_test_nonan = []\n",
    "    for i in range(3):\n",
    "        mask = ~np.isnan(data[1][0,...,i])\n",
    "        interp_test_nonan.append(data[1][...,i][:,mask])\n",
    "    f_interp = []\n",
    "    for i in range(3):\n",
    "        _interp = np.einsum('t n -> t', np.abs(np.fft.fft(interp_test_nonan[i]-np.mean(interp_test_nonan[i]),axis=0)))\n",
    "        # _interp = _interp / np.std(_interp)\n",
    "        f_interp.append(_interp)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1,3,sharex=True,figsize=(7,2.5))\n",
    "    for i in range(3):\n",
    "        axes[i].plot(fftfreq[:midpoint], 2*f_data[:midpoint,i],label='True',linewidth=3,color='#808080',alpha=0.5)\n",
    "        axes[i].plot(fftfreq[:midpoint], 2*f_interp[i][:midpoint],label='Interp',linewidth=1,color='k',linestyle='--')\n",
    "        \n",
    "        f_pred = np.einsum('t x y u -> t u', np.abs(np.fft.fft(data[2]-np.mean(data[2],axis=0),axis=0)))\n",
    "        axes[i].plot(fftfreq[:midpoint], 2*f_pred[:midpoint,i],label='Reconstructed',linewidth=1, color=my_discrete_cmap(0))\n",
    "        axes[i].set(xlabel='$Hz$',title=var_names[i])\n",
    "        \n",
    "    axes[0].set(ylabel='Magnitude', xlim=[0,4])\n",
    "        \n",
    "\n",
    "    handlers = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(*handlers,loc='upper center', bbox_to_anchor=(0.5, 1.15),ncols=5)\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_image_freq(results,'2dtriangle_clean_freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = [[44, 44, 23, 23, 140, 68, 40, 103, 5, 5, 20, 20, 150, 175, 200, 225],[81, 47, 51, 77, 64, 64, 64, 64, 80, 48, 78, 50, 10, 119, 10, 119]]\n",
    "fig = plt.figure(figsize=(1.5,5))\n",
    "sfig = fig.subfigures(1,1,facecolor='lightsteelblue')\n",
    "subfigs = fig.subfigures(1,2,width_ratios=[0.05,0.9],wspace=-0.1,facecolor='lightsteelblue')\n",
    "axes = subfigs[1].subplots(3,1)\n",
    "for i in range(3):\n",
    "    axes[i].imshow(results[0][0,:,:,i],alpha=0.3)\n",
    "    axes[i].axis('off')\n",
    "    # axes[i].scatter(idx[1],idx[0],s=3,c='k',marker='s')\n",
    "    axes[i].spy(observed[0,...,i],color='k',marker='s',markersize=1,alpha=0.6)\n",
    "axes[2].scatter(range(49,80),[0]*31,marker='s',c='r',s=4, zorder=5)\n",
    "\n",
    "subfigs[1].text(-0.032,0.5,'$\\\\xi(\\mathbf{D})$')\n",
    "\n",
    "# fig.savefig('./figs/data_taking_observations',bbox_inches='tight')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1, figsize=(3.5,2))\n",
    "ax.imshow(results[0][time,...,-1].T, alpha=0.3)\n",
    "ax.spy(observed[0,...,-1].T,color='r',marker='s',markersize=2,alpha=0.6)\n",
    "ax.spy(observed[0,...,0].T,color='k',marker='s',markersize=2,zorder=5)\n",
    "ax.set(xticks=[],xlabel='$x_1$',yticks=[],ylabel='$x_2$')\n",
    "fig.savefig('./figs/2dtriangle_clean_sensor_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_interp = losses.relative_error(results[1],results[2])*100\n",
    "loss_physics_interp = losses.divergence(results[1][...,:-1],datainfo) + losses.momentum_loss(results[1],datainfo)\n",
    "loss_physics_ref = losses.divergence(results[0][...,:-1],datainfo) + losses.momentum_loss(results[0],datainfo)\n",
    "\n",
    "print(f'Interpolated relative error(%) from sensors is {loss_interp}')\n",
    "print(f'Interpolated physics loss from sensors is {loss_physics_interp}')\n",
    "print(f'Reference data has physics loss {loss_physics_ref}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
