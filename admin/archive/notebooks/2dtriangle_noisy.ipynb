{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import h5py\n",
    "import jax\n",
    "import yaml\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../flowrec/utils/a4.mplstyle')\n",
    "\n",
    "import flowrec.data as data_utils\n",
    "import flowrec.physics_and_derivatives as derivatives\n",
    "import flowrec.training_and_states as state_utils\n",
    "\n",
    "from ml_collections import config_dict\n",
    "from pathlib import Path\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from flowrec import losses\n",
    "from flowrec.utils import simulation, my_discrete_cmap\n",
    "from flowrec.utils.py_helper import slice_from_tuple\n",
    "from flowrec.utils.system import set_gpu\n",
    "set_gpu(1,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = Path('../local_results/2dtriangle/repeat_noisy/noisy_random')\n",
    "if not result_dir.exists():\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_one_case(folder):\n",
    "    with h5py.File(Path(result_dir,folder,'summary.h5')) as hf:\n",
    "        l_train = np.array(hf.get('runs_loss_train'))\n",
    "        l_val = np.array(hf.get('runs_loss_val'))\n",
    "        idx = np.argmin(np.sum(l_train[:,1:],axis=-1))\n",
    "        best_run = np.array(hf.get('runs_name')).astype('unicode')[idx]\n",
    "    # best_run_path = Path(result_dir,folder,best_run)\n",
    "    \n",
    "    lmean = np.array([np.mean(l_train[:,0]), np.mean(l_val[:,0])]) # mean over the repeats [rel_l2 train, rel_l2 val]\n",
    "    lstd = np.array([np.std(l_train[:,0]), np.std(l_val[:,0])]) # std over the repeats [rel_l2 train, rel_l2 val]\n",
    "    lpmean = np.array([\n",
    "        np.mean(np.sum(l_train[:,1:3],axis=1)),\n",
    "        np.mean(np.sum(l_val[:,1:3],axis=1))\n",
    "    ])\n",
    "    lpstd = np.array([\n",
    "        np.std(np.sum(l_train[:,1:3],axis=1)),\n",
    "        np.std(np.sum(l_val[:,1:3],axis=1))\n",
    "    ])\n",
    "    print(best_run)\n",
    " \n",
    "    return lmean, lstd, lpmean, lpstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load summary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmean_snr20_classic, lstd_snr20_classic, lpmean_snr20_classic, lpstd_snr20_classic = get_summary_one_case('snr20_classic')\n",
    "lmean_snr20_3, lstd_snr20_3, lpmean_snr20_3, lpstd_snr20_3 = get_summary_one_case('snr20_3')\n",
    "lmean_snr20_mean3, lstd_snr20_mean3, lpmean_snr20_mean3, lpstd_snr20_mean3 = get_summary_one_case('snr20_mean3')\n",
    "\n",
    "lmean_snr10_classic, lstd_snr10_classic, lpmean_snr10_classic, lpstd_snr10_classic = get_summary_one_case('snr10_classic')\n",
    "lmean_snr10_3, lstd_snr10_3, lpmean_snr10_3, lpstd_snr10_3 = get_summary_one_case('snr10_3')\n",
    "lmean_snr10_mean3, lstd_snr10_mean3, lpmean_snr10_mean3, lpstd_snr10_mean3, = get_summary_one_case('snr10_mean3')\n",
    "\n",
    "lmean_snr5_classic, lstd_snr5_classic, lpmean_snr5_classic, lpstd_snr5_classic = get_summary_one_case('snr5_classic')\n",
    "lmean_snr5_3, lstd_snr5_3, lpmean_snr5_3, lpstd_snr5_3 = get_summary_one_case('snr5_3')\n",
    "lmean_snr5_mean3, lstd_snr5_mean3, lpmean_snr5_mean3, lpstd_snr5_mean3 = get_summary_one_case('snr5_mean3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmean_snr20_classic, lstd_snr20_classic, lpmean_snr20_classic, lpstd_snr20_classic = get_summary_one_case('snr20_classic')\n",
    "lmean_snr20_3, lstd_snr20_3, lpmean_snr20_3, lpstd_snr20_3 = get_summary_one_case('snr20_3')\n",
    "lmean_snr20_mean3, lstd_snr20_mean3, lpmean_snr20_mean3, lpstd_snr20_mean3 = get_summary_one_case('snr20_mean3')\n",
    "\n",
    "lmean_snr10_classic, lstd_snr10_classic, lpmean_snr10_classic, lpstd_snr10_classic = get_summary_one_case('snr10_classic')\n",
    "lmean_snr10_3, lstd_snr10_3, lpmean_snr10_3, lpstd_snr10_3 = get_summary_one_case('snr10_3')\n",
    "lmean_snr10_mean3, lstd_snr10_mean3, lpmean_snr10_mean3, lpstd_snr10_mean3, = get_summary_one_case('snr10_mean3')\n",
    "\n",
    "lmean_snr5_classic, lstd_snr5_classic, lpmean_snr5_classic, lpstd_snr5_classic = get_summary_one_case('snr5_classic')\n",
    "lmean_snr5_3, lstd_snr5_3, lpmean_snr5_3, lpstd_snr5_3 = get_summary_one_case('snr5_3')\n",
    "lmean_snr5_mean3, lstd_snr5_mean3, lpmean_snr5_mean3, lpstd_snr5_mean3 = get_summary_one_case('snr5_mean3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_seed = str(325)\n",
    "\n",
    "run_snr20_classic = Path(result_dir,'snr20_classic/classic-'+best_run_seed)\n",
    "run_snr10_classic = Path(result_dir,'snr10_classic/classic-'+best_run_seed)\n",
    "run_snr5_classic = Path(result_dir,'snr5_classic/classic-'+best_run_seed)\n",
    "run_snr20_3 = Path(result_dir,'snr20_3/3-'+best_run_seed)\n",
    "run_snr10_3 = Path(result_dir,'snr10_3/3-'+best_run_seed)\n",
    "run_snr5_3 = Path(result_dir,'snr5_3/3-'+best_run_seed)\n",
    "run_snr20_mean3 = Path(result_dir,'snr20_mean3/mean3-'+best_run_seed)\n",
    "run_snr10_mean3 = Path(result_dir,'snr10_mean3/mean3-'+best_run_seed)\n",
    "run_snr5_mean3 = Path(result_dir,'snr5_mean3/mean3-'+best_run_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare relative error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_mean_classic = []\n",
    "l_mean_classic.append(lmean_snr20_classic)\n",
    "l_mean_classic.append(lmean_snr10_classic)\n",
    "l_mean_classic.append(lmean_snr5_classic)\n",
    "l_mean_classic = np.array(l_mean_classic)*100\n",
    "l_std_classic = []\n",
    "l_std_classic.append(lstd_snr20_classic)\n",
    "l_std_classic.append(lstd_snr10_classic)\n",
    "l_std_classic.append(lstd_snr5_classic)\n",
    "l_std_classic = np.array(l_std_classic)*100\n",
    "\n",
    "l_mean_3 = []\n",
    "l_mean_3.append(lmean_snr20_3)\n",
    "l_mean_3.append(lmean_snr10_3)\n",
    "l_mean_3.append(lmean_snr5_3)\n",
    "l_mean_3 = np.array(l_mean_3)*100\n",
    "l_std_3 = []\n",
    "l_std_3.append(lstd_snr20_3)\n",
    "l_std_3.append(lstd_snr10_3)\n",
    "l_std_3.append(lstd_snr5_3)\n",
    "l_std_3 = np.array(l_std_3)*100\n",
    "\n",
    "l_mean_mean3 = []\n",
    "l_mean_mean3.append(lmean_snr20_mean3)\n",
    "l_mean_mean3.append(lmean_snr10_mean3)\n",
    "l_mean_mean3.append(lmean_snr5_mean3)\n",
    "l_mean_mean3 = np.array(l_mean_mean3)*100\n",
    "l_std_mean3 = []\n",
    "l_std_mean3.append(lstd_snr20_mean3)\n",
    "l_std_mean3.append(lstd_snr10_mean3)\n",
    "l_std_mean3.append(lstd_snr5_mean3)\n",
    "l_std_mean3 = np.array(l_std_mean3)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l_mean_classic - rows: 20,10,5 snr. columns: training,testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = [20,10,5]\n",
    "plt.figure(figsize=(5,3))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "ax.errorbar(snr,l_mean_classic[:,0],yerr=l_std_classic[:,0],label='$\\mathcal{L}^c$ train',marker='x',color=my_discrete_cmap(0),linewidth=2.5)\n",
    "ax.errorbar(snr,l_mean_3[:,0],yerr=l_std_3[:,0],label='$\\mathcal{L}^s$ train',marker='x',color=my_discrete_cmap(1),linewidth=2.5)\n",
    "ax.errorbar(snr,l_mean_mean3[:,0],yerr=l_std_mean3[:,0],label='$\\mathcal{L}^m$ train',marker='x',color=my_discrete_cmap(2),linewidth=2.5)\n",
    "\n",
    "ax.errorbar(snr,l_mean_classic[:,1],yerr=l_std_classic[:,1],label='$\\mathcal{L}^c$ test',marker='.',color='deepskyblue',linestyle=':')\n",
    "ax.errorbar(snr,l_mean_3[:,1],yerr=l_std_3[:,1],label='$\\mathcal{L}^s$ test',marker='.',linestyle=':',color='limegreen')\n",
    "ax.errorbar(snr,l_mean_mean3[:,1],yerr=l_std_mean3[:,1],label='$\\mathcal{L}^m$ test',marker='.',linestyle=':',color='darkorange',)\n",
    "\n",
    "ax.legend(ncol=2,loc='upper right')\n",
    "ax.set_ylabel('$\\epsilon (\\%)$')\n",
    "ax.set_xticks([5,10,20])\n",
    "ax.set_xlabel('SNR')\n",
    "# ax.set_yscale('log')\n",
    "# plt.ylim([0,70])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Mean and standard deviatin of the rel-L2 for loss classic, strict and mean at')\n",
    "print(f'SNR20: {l_mean_classic[0,0]:.2f}+-{l_std_classic[0,0]:.4f}, {l_mean_3[0,0]:.3f}+-{l_std_3[0,0]:.4f}, {l_mean_mean3[0,0]:.3f}+-{l_std_mean3[0,0]:.4f}')\n",
    "print(f'SNR10: {l_mean_classic[1,0]:.2f}+-{l_std_classic[1,0]:.4f}, {l_mean_3[1,0]:.3f}+-{l_std_3[1,0]:.4f}, {l_mean_mean3[1,0]:.3f}+-{l_std_mean3[1,0]:.4f}')\n",
    "print(f'SNR5: {l_mean_classic[2,0]:.2f}+-{l_std_classic[2,0]:.4f}, {l_mean_3[2,0]:.3f}+-{l_std_3[2,0]:.4f}, {l_mean_mean3[2,0]:.3f}+-{l_std_mean3[2,0]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare physics loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_mean_classic = []\n",
    "lp_mean_classic.append(lpmean_snr20_classic)\n",
    "lp_mean_classic.append(lpmean_snr10_classic)\n",
    "lp_mean_classic.append(lpmean_snr5_classic)\n",
    "lp_mean_classic = np.array(lp_mean_classic)\n",
    "lp_std_classic = []\n",
    "lp_std_classic.append(lpstd_snr20_classic)\n",
    "lp_std_classic.append(lpstd_snr10_classic)\n",
    "lp_std_classic.append(lpstd_snr5_classic)\n",
    "lp_std_classic = np.array(lp_std_classic)\n",
    "\n",
    "lp_mean_3 = []\n",
    "lp_mean_3.append(lpmean_snr20_3)\n",
    "lp_mean_3.append(lpmean_snr10_3)\n",
    "lp_mean_3.append(lpmean_snr5_3)\n",
    "lp_mean_3 = np.array(lp_mean_3)\n",
    "lp_std_3 = []\n",
    "lp_std_3.append(lpstd_snr20_3)\n",
    "lp_std_3.append(lpstd_snr10_3)\n",
    "lp_std_3.append(lpstd_snr5_3)\n",
    "lp_std_3 = np.array(lp_std_3)\n",
    "\n",
    "lp_mean_mean3 = []\n",
    "lp_mean_mean3.append(lpmean_snr20_mean3)\n",
    "lp_mean_mean3.append(lpmean_snr10_mean3)\n",
    "lp_mean_mean3.append(lpmean_snr5_mean3)\n",
    "lp_mean_mean3 = np.array(lp_mean_mean3)\n",
    "lp_std_mean3 = []\n",
    "lp_std_mean3.append(lpstd_snr20_mean3)\n",
    "lp_std_mean3.append(lpstd_snr10_mean3)\n",
    "lp_std_mean3.append(lpstd_snr5_mean3)\n",
    "lp_std_mean3 = np.array(lp_std_mean3)\n",
    "\n",
    "lp_ref = 0.043 # this is the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr = [20,10,5]\n",
    "plt.figure(figsize=(5.5,3.5))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# training set\n",
    "ax.errorbar(snr,lp_mean_classic[:,0],yerr=lp_std_classic[:,0],label='$\\mathcal{L}^c$ training',marker='x',color=my_discrete_cmap(0),linewidth=2.5)\n",
    "ax.errorbar(snr,lp_mean_3[:,0],yerr=lp_std_3[:,0],label='$\\mathcal{L}^s$ training',marker='x',color=my_discrete_cmap(1),linewidth=2.5)\n",
    "ax.errorbar(snr,lp_mean_mean3[:,0],yerr=lp_std_mean3[:,0],label='$\\mathcal{L}^m$ training',marker='x',color=my_discrete_cmap(2),linewidth=2.5)\n",
    "\n",
    "# # validation set\n",
    "# ax.errorbar(snr,lp_mean_classic[:,1],yerr=lp_std_classic[:,1],label='$\\mathcal{L}^c$ testing',marker='.',color='deepskyblue',linestyle=':')\n",
    "# ax.errorbar(snr,lp_mean_3[:,1],yerr=lp_std_3[:,1],label='$\\mathcal{L}^s$ testing',marker='.',linestyle=':',color='darkorange')\n",
    "# ax.errorbar(snr,lp_mean_mean3[:,1],yerr=lp_std_mean3[:,1],label='$\\mathcal{L}^m$ testing',marker='.',linestyle=':',color='limegreen',)\n",
    "\n",
    "ax.hlines(lp_ref, xmin=5,xmax=20, colors=['k'], linestyles='dashed',label='reference data')\n",
    "ax.legend(ncol=1)\n",
    "ax.set_ylabel('$\\mathcal{L}_p$')\n",
    "ax.set_xticks([5,10,20])\n",
    "ax.set_xlabel('SNR')\n",
    "# ax.set_yscale('log')\n",
    "# plt.ylim([0,70])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Mean and standard deviatin of the physics loss for loss classic, strict and mean at')\n",
    "print(f'SNR20: {lp_mean_classic[0,0]:.2f}+-{lp_std_classic[0,0]:.4f}, {lp_mean_3[0,0]:.3f}+-{lp_std_3[0,0]:.4f}, {lp_mean_mean3[0,0]:.3f}+-{lp_std_mean3[0,0]:.4f}')\n",
    "print(f'SNR10: {lp_mean_classic[1,0]:.2f}+-{lp_std_classic[1,0]:.4f}, {lp_mean_3[1,0]:.3f}+-{lp_std_3[1,0]:.4f}, {lp_mean_mean3[1,0]:.3f}+-{lp_std_mean3[1,0]:.4f}')\n",
    "print(f'SNR5: {lp_mean_classic[2,0]:.2f}+-{lp_std_classic[2,0]:.4f}, {lp_mean_3[2,0]:.3f}+-{lp_std_3[2,0]:.4f}, {lp_mean_mean3[2,0]:.3f}+-{lp_std_mean3[2,0]:.4f}')\n",
    "print(f'Physics loss of the reference data is {lp_ref:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axl, axr) = plt.subplots(1,2,figsize=(7,2.5), sharex=True)\n",
    "\n",
    "axl.errorbar(snr,l_mean_classic[:,0],yerr=l_std_classic[:,0],label='$\\mathcal{L}^c$ ',marker='x',color=my_discrete_cmap(0),linewidth=2.5)\n",
    "axl.errorbar(snr,l_mean_3[:,0],yerr=l_std_3[:,0],label='$\\mathcal{L}^s$ ',marker='x',color=my_discrete_cmap(1),linewidth=2.5)\n",
    "axl.errorbar(snr,l_mean_mean3[:,0],yerr=l_std_mean3[:,0],label='$\\mathcal{L}^m$ ',marker='x',color=my_discrete_cmap(2),linewidth=2.5)\n",
    "\n",
    "# axl.errorbar(snr,l_mean_classic[:,1],yerr=l_std_classic[:,1],label='$\\mathcal{L}^c$ test',marker='.',color='deepskyblue',linestyle=':')\n",
    "# axl.errorbar(snr,l_mean_3[:,1],yerr=l_std_3[:,1],label='$\\mathcal{L}^s$ test',marker='.',linestyle=':',color='limegreen')\n",
    "# axl.errorbar(snr,l_mean_mean3[:,1],yerr=l_std_mean3[:,1],label='$\\mathcal{L}^m$ test',marker='.',linestyle=':',color='darkorange',)\n",
    "\n",
    "axl.set_ylabel('$\\epsilon (\\%)$')\n",
    "axl.set_xticks([5,10,20])\n",
    "axl.set_xlabel('SNR')\n",
    "\n",
    "axr.errorbar(snr,lp_mean_classic[:,0],yerr=lp_std_classic[:,0],marker='x',color=my_discrete_cmap(0),linewidth=2.5)\n",
    "axr.errorbar(snr,lp_mean_3[:,0],yerr=lp_std_3[:,0], marker='x',color=my_discrete_cmap(1),linewidth=2.5)\n",
    "axr.errorbar(snr,lp_mean_mean3[:,0],yerr=lp_std_mean3[:,0],marker='x',color=my_discrete_cmap(2),linewidth=2.5)\n",
    "axr.hlines(lp_ref, xmin=5,xmax=20, colors=['k'], linestyles='dashed',label='reference data')\n",
    "# axr.legend(ncol=1)\n",
    "axr.set_ylabel('$\\mathcal{L}_p$')\n",
    "axr.set_xticks([5,10,20])\n",
    "axr.set_xlabel('SNR')\n",
    "\n",
    "fig.legend(ncol=4,loc='upper center', bbox_to_anchor=(0.5, 1.05))\n",
    "# fig.savefig('./figs/2dtriangle_noisy_compare_lossfn',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare flowfields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(u,pb,case_observe,datacfg):\n",
    "\n",
    "    take_observation, insert_observation = case_observe(datacfg, example_pred_snapshot=u[0,...],example_pin_snapshot=pb[0,...])\n",
    "    observed = take_observation(u)\n",
    "    temp_observed = np.empty_like(u)\n",
    "    temp_observed.fill(np.nan) #this is noisy\n",
    "    temp_observed = insert_observation(jnp.asarray(temp_observed),jnp.asarray(observed)) # observed_test is noisy if\n",
    "\n",
    "    # get sensor coordinates\n",
    "    sensors_empty = np.empty_like(u[[0],...])\n",
    "    sensors_empty.fill(np.nan)\n",
    "\n",
    "    grid_x,grid_y = np.mgrid[0:u[...,0].shape[1], 0:u[...,0].shape[2]]\n",
    "\n",
    "    gridx1 = np.repeat(grid_x[None,:,:,None],3,axis=3)\n",
    "    gridy1 = np.repeat(grid_y[None,:,:,None],3,axis=3)\n",
    "\n",
    "    idx_x = take_observation(gridx1)\n",
    "    idx_y = take_observation(gridy1)\n",
    "\n",
    "    idx_x = insert_observation(jnp.asarray(sensors_empty),jnp.asarray(idx_x))[0,...]\n",
    "    sensors_loc_x = []\n",
    "    for i in range(idx_x.shape[-1]):\n",
    "        sensors_loc_x.append(idx_x[...,i][~np.isnan(idx_x[...,i])])\n",
    "\n",
    "    idx_y = insert_observation(jnp.asarray(sensors_empty),jnp.asarray(idx_y))[0,...]\n",
    "    sensors_loc_y = []\n",
    "    for i in range(idx_y.shape[-1]):\n",
    "        sensors_loc_y.append(idx_y[...,i][~np.isnan(idx_y[...,i])])\n",
    "\n",
    "\n",
    "    compare_interp = list([])\n",
    "    nt = u.shape[0]\n",
    "    _locs = np.stack((grid_x.flatten(),grid_y.flatten()),axis=-1)\n",
    "\n",
    "    print('Starting interpolation')\n",
    "    for i in range(3):\n",
    "        print(f\"Component {i}\")\n",
    "        sensors_loc = np.stack((sensors_loc_x[i].flatten(),sensors_loc_y[i].flatten()),axis=-1)\n",
    "        for j in range(nt):\n",
    "            if j%50 == 0:\n",
    "                print(j)\n",
    "            temp_measurement = temp_observed[j,...,i][~np.isnan(temp_observed[j,...,i])]\n",
    "            # print(sensors_loc.shape, temp_measurement.shape)\n",
    "            rbf = RBFInterpolator(sensors_loc,temp_measurement.flatten(),kernel='thin_plate_spline')\n",
    "            _interp = rbf(_locs).reshape(grid_x.shape)\n",
    "            compare_interp.append(_interp)\n",
    "    compare_interp = np.array(compare_interp)\n",
    "    compare_interp = np.stack((compare_interp[:nt,...],compare_interp[nt:2*nt,...],compare_interp[2*nt:3*nt,...]),axis=-1)\n",
    "\n",
    "    return compare_interp, temp_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_case_predictions(results_dir, predict_only=False):\n",
    "    print(f\"Starting {results_dir}\")\n",
    "    with open(Path(results_dir,'config.yml'),'r') as f:\n",
    "        cfg = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "    cfg.data_config.update({'data_dir':'.'+cfg.data_config.data_dir})\n",
    "    x_base = 132\n",
    "    triangle_base_coords = [49,80]\n",
    "    (ux,uy,pp) = simulation.read_data_2dtriangle(cfg.data_config.data_dir,x_base)\n",
    "    x = np.stack([ux,uy,pp],axis=0)\n",
    "    # remove parts where uz is not zero\n",
    "    s = slice_from_tuple(cfg.data_config.slice_to_keep)\n",
    "    x = x[s]\n",
    "\n",
    "    # information about the grid\n",
    "    datainfo = data_utils.DataMetadata(\n",
    "        re = cfg.data_config.re,\n",
    "        discretisation=[cfg.data_config.dt,cfg.data_config.dx,cfg.data_config.dy],\n",
    "        axis_index=[0,1,2],\n",
    "        problem_2d=True\n",
    "    ).to_named_tuple()\n",
    "\n",
    "    rng = np.random.default_rng(cfg.data_config.randseed)\n",
    "    if cfg.data_config.snr:\n",
    "        [x_train,x_val,x_test], _ = data_utils.data_partition(x,1,cfg.data_config.train_test_split,REMOVE_MEAN=cfg.data_config.remove_mean,randseed=cfg.data_config.randseed,SHUFFLE=cfg.data_config.shuffle) # Do not shuffle, do not remove mean for training with physics informed loss\n",
    "        [ux_train,uy_train,pp_train] = np.squeeze(np.split(x_train,3,axis=0))\n",
    "        # [ux_val,uy_val,pp_val] = np.squeeze(np.split(x_val,3,axis=0))\n",
    "        # [ux_test,uy_test,pp_test] = np.squeeze(np.split(x_test,3,axis=0))\n",
    "        u_train = np.stack((ux_train,uy_train,pp_train),axis=-1)\n",
    "        # u_val = np.stack((ux_val,uy_val,pp_val),axis=-1)\n",
    "        # u_test = np.stack((ux_test,uy_test,pp_test),axis=-1)\n",
    "\n",
    "        \n",
    "        std_data = np.std(x,axis=(1,2,3),ddof=1)\n",
    "        std_n = data_utils.get_whitenoise_std(cfg.data_config.snr,std_data)\n",
    "        noise_ux = rng.normal(scale=std_n[0],size=x[0,...].shape)\n",
    "        noise_uy = rng.normal(scale=std_n[1],size=x[1,...].shape)\n",
    "        noise_pp = rng.normal(scale=std_n[2],size=x[2,...].shape)\n",
    "        noise = np.stack([noise_ux,noise_uy,noise_pp],axis=0)\n",
    "        x = x + noise\n",
    "\n",
    "\n",
    "    [x_train_n,x_val_n,x_test_n], _ = data_utils.data_partition(\n",
    "        x,\n",
    "        1,\n",
    "        cfg.data_config.train_test_split,\n",
    "        REMOVE_MEAN=cfg.data_config.remove_mean,\n",
    "        randseed=cfg.data_config.randseed,\n",
    "        SHUFFLE=cfg.data_config.shuffle\n",
    "    ) # Do not shuffle, do not remove mean for training with physics informed loss\n",
    "    [ux_train_n,uy_train_n,pp_train_n] = np.squeeze(np.split(x_train_n,3,axis=0))\n",
    "    [ux_val_n,uy_val_n,pp_val_n] = np.squeeze(np.split(x_val_n,3,axis=0))\n",
    "    [ux_test_n,uy_test_n,pp_test_n] = np.squeeze(np.split(x_test_n,3,axis=0))\n",
    "    u_train_n = np.stack((ux_train_n,uy_train_n,pp_train_n),axis=-1)\n",
    "    u_val_n = np.stack((ux_val_n,uy_val_n,pp_val_n),axis=-1)\n",
    "    u_test_n = np.stack((ux_test_n,uy_test_n,pp_test_n),axis=-1)\n",
    "\n",
    "    pb_train = simulation.take_measurement_base(pp_train_n,ly=triangle_base_coords,centrex=0).reshape((cfg.data_config.train_test_split[0],-1))\n",
    "    pb_val = simulation.take_measurement_base(pp_val_n,ly=triangle_base_coords,centrex=0).reshape((cfg.data_config.train_test_split[1],-1))\n",
    "    pb_test = simulation.take_measurement_base(pp_test_n,ly=triangle_base_coords,centrex=0).reshape((cfg.data_config.train_test_split[2],-1))\n",
    "\n",
    "    take_observation, insert_observation = cfg.case.observe(cfg.data_config, example_pred_snapshot=u_train_n[0,...],example_pin_snapshot=pb_train[0,...])\n",
    "    observed_train, train_minmax = take_observation(u_train_n,init=True)\n",
    "    observed_val, val_minmax = take_observation(u_val_n,init=True)\n",
    "    observed_test, test_minmax = take_observation(u_test_n,init=True)\n",
    "    \n",
    "    state = state_utils.restore_trainingstate(results_dir,'state')\n",
    "    _, make_model = cfg.case.select_model(datacfg=cfg.data_config, mdlcfg=cfg.model_config, traincfg=cfg.train_config)\n",
    "    mdl = make_model(cfg.model_config)\n",
    "\n",
    "    if cfg.data_config.normalise:\n",
    "        [pb_train, pb_val, pb_test], _ = data_utils.normalise(pb_train, pb_val, pb_test, range=[train_minmax[-1],val_minmax[-1],test_minmax[-1]])\n",
    "\n",
    "    rng = jax.random.PRNGKey(10)\n",
    "\n",
    "    pb_train_batch = np.array_split(pb_train,2,0)\n",
    "    pred_train = []\n",
    "    for inn in pb_train_batch:\n",
    "        pred_train.append(mdl.apply(state.params,rng,inn,TRAINING=False))\n",
    "    pred_train = np.concatenate(pred_train)\n",
    "    # pred_test = mdl.apply(state.params,rng,pb_test,TRAINING=False)\n",
    "    if cfg.data_config.normalise:\n",
    "        pred_train = data_utils.unnormalise_group(pred_train, train_minmax, axis_data=-1, axis_range=0)\n",
    "        # pred_test = data_utils.unnormalise_group(pred_test, test_minmax, axis_data=-1, axis_range=0)\n",
    "    \n",
    "    if predict_only:\n",
    "        return pred_train \n",
    "\n",
    "    u_interp, observed = interpolate(u_train_n, pb_train, cfg.case.observe, cfg.data_config)\n",
    "    \n",
    "    print(f'Finished {results_dir}')\n",
    "\n",
    "    return (u_train ,u_train_n, u_interp, pred_train), datainfo, observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_snapshots(data, datainfo, figname, t1):\n",
    "    # data is (ref, noisy, interp, classic, loss3, mean3)\n",
    "\n",
    "    ref = data[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,4))\n",
    "    \n",
    "    # grids for mean\n",
    "    grid_b1 = ImageGrid(fig, (0.08,0.00,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b2 = ImageGrid(fig, (0.08,0.15,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b3 = ImageGrid(fig, (0.08,0.30,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # grids for snapshots\n",
    "    grid_t1 = ImageGrid(fig, (0.08,0.50,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t2 = ImageGrid(fig, (0.08,0.65,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t3 = ImageGrid(fig, (0.08,0.80,0.92,0.14), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # snapshots\n",
    "    for i, grid in enumerate([grid_t3,grid_t2,grid_t1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(ref[t1,...,i].T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(data[j][t1,...,i].T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    # mean\n",
    "    for i, grid in enumerate([grid_b3,grid_b2,grid_b1]):\n",
    "        axes = grid.axes_all\n",
    "        im_ref = axes[0].imshow(np.mean(ref[...,i],axis=0).T)\n",
    "        \n",
    "        for j in range(1,6):\n",
    "            im = axes[j].imshow(np.mean(data[j][...,i],axis=0).T)\n",
    "            im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "        grid.cbar_axes[0].colorbar(im_ref)\n",
    "        grid.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    fig.text(0.11,0.98,'Reference')\n",
    "    fig.text(0.28,0.98,'Noisy')\n",
    "    fig.text(0.41,0.98,'Interpolated')\n",
    "    fig.text(0.60,0.98,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.75,0.98,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.9,0.98,'$\\mathcal{L}^m$')\n",
    "    fig.text(0.01,0.20,'Mean',rotation='vertical')\n",
    "    fig.text(0.05,0.07, '$\\overline{p}$')\n",
    "    fig.text(0.05,0.22, '$\\overline{u}_2$')\n",
    "    fig.text(0.05,0.37, '$\\overline{u}_1$')\n",
    "    fig.text(0.01,0.70,f't={t1*datainfo.dt:.1f}',rotation='vertical')\n",
    "    fig.text(0.05,0.57, '$p$')\n",
    "    fig.text(0.05,0.72, '$u_2$')\n",
    "    fig.text(0.05,0.87, '$u_1$')\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_snapshots_vorticity(data, datainfo, figname, t1):\n",
    "    # data is (ref, noisy, interp, classic, loss3, mean3)\n",
    "\n",
    "    ref = data[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(7,2.5))\n",
    "    \n",
    "    # grids for mean\n",
    "    grid_b1 = ImageGrid(fig, (0.08,0.00,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_b2 = ImageGrid(fig, (0.08,0.23,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    \n",
    "    # grids for snapshots\n",
    "    grid_t1 = ImageGrid(fig, (0.08,0.52,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "    grid_t2 = ImageGrid(fig, (0.08,0.75,0.92,0.22), (1,6),cbar_mode='single', share_all=True)\n",
    "\n",
    "\n",
    "    data_vort = []\n",
    "    for _data in data:\n",
    "        data_vort.append(\n",
    "            derivatives.vorticity(_data[...,:2], datainfo)\n",
    "        )\n",
    "    \n",
    "    # snapshots\n",
    "    axes = grid_t2.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        data_vort[0][t1,...].T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            data_vort[j][t1,...].T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_t2.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_t2.axes_all[0].set(xticks=[],yticks=[])\n",
    "    axes = grid_t1.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        ref[t1,...,2].T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            data[j][t1,...,2].T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_t1.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_t1.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    # mean\n",
    "    axes = grid_b2.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        np.mean(data_vort[0],axis=0).T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            np.mean(data_vort[j],axis=0).T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_b2.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_b2.axes_all[0].set(xticks=[],yticks=[])\n",
    "    axes = grid_b1.axes_all\n",
    "    im_ref = axes[0].imshow(\n",
    "        np.mean(ref[...,2],axis=0).T\n",
    "    )\n",
    "    for j in range(1,6):\n",
    "        im = axes[j].imshow(\n",
    "            np.mean(data[j][...,2],axis=0).T\n",
    "        )\n",
    "        im.set_clim(im_ref.get_clim()[0],im_ref.get_clim()[1])\n",
    "    grid_b1.cbar_axes[0].colorbar(im_ref)\n",
    "    grid_b1.axes_all[0].set(xticks=[],yticks=[])\n",
    "    \n",
    "    \n",
    "    fig.text(0.11,0.98,'Reference')\n",
    "    fig.text(0.28,0.98,'Noisy')\n",
    "    fig.text(0.41,0.98,'Interpolated')\n",
    "    fig.text(0.60,0.98,'$\\mathcal{L}^c$')\n",
    "    fig.text(0.75,0.98,'$\\mathcal{L}^s$')\n",
    "    fig.text(0.9,0.98,'$\\mathcal{L}^m$')\n",
    "    fig.text(0.005,0.20,'Mean',rotation='vertical')\n",
    "    fig.text(0.05,0.1, '$\\overline{p}$')\n",
    "    fig.text(0.05,0.3, '$\\overline{v}$')\n",
    "    fig.text(0.005,0.70,f't={t1*datainfo.dt:.1f}',rotation='vertical')\n",
    "    fig.text(0.05,0.6, '$p$')\n",
    "    fig.text(0.05,0.85, '$v$')\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = ['$u_1^\\prime$','$u_2^\\prime$','$p^\\prime$']\n",
    "\n",
    "def make_image_freq(data, figname, dt):\n",
    "    # data is (ref, noisy, interp, classic, loss3, mean3)\n",
    "\n",
    "    n = np.prod(data[0].shape[1:-1])\n",
    "    fftfreq = np.fft.fftfreq(data[0].shape[0],d = dt)\n",
    "    midpoint = int(len(fftfreq)/2)\n",
    "    \n",
    "    # data\n",
    "    f_data = np.einsum('t x y u -> t u', np.abs(np.fft.fft(data[0]-np.mean(data[0],axis=0),axis=0)))\n",
    "\n",
    "    # interpolated\n",
    "    interp_test_nonan = []\n",
    "    for i in range(3):\n",
    "        mask = ~np.isnan(data[2][0,...,i])\n",
    "        interp_test_nonan.append(data[2][...,i][:,mask])\n",
    "    f_interp = []\n",
    "    for i in range(3):\n",
    "        _interp = np.einsum('t n -> t', np.abs(np.fft.fft(interp_test_nonan[i]-np.mean(interp_test_nonan[i]),axis=0)))\n",
    "        # _interp = _interp / np.std(_interp)\n",
    "        f_interp.append(_interp)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(1,3,sharex=True,figsize=(7,3))\n",
    "    for i in range(3):\n",
    "        axes[i].plot(fftfreq[:midpoint], 2*f_data[:midpoint,i],label='Reference',linewidth=3,color='#808080',alpha=0.5)\n",
    "        axes[i].plot(fftfreq[:midpoint], 2*f_interp[i][:midpoint],label='Interpolated',linewidth=1,color='k',linestyle='--')\n",
    "        \n",
    "        for j,lossfn in enumerate(['$\\mathcal{L}^c$','$\\mathcal{L}^s$','$\\mathcal{L}^m$']):\n",
    "            f_pred = np.einsum('t x y u -> t u', np.abs(np.fft.fft(data[j+3]-np.mean(data[j+3],axis=0),axis=0)))\n",
    "            axes[i].plot(fftfreq[:midpoint], 2*f_pred[:midpoint,i],label=lossfn,linewidth=1,color=my_discrete_cmap(j))\n",
    "        axes[i].set(xlabel='Hz',title=var_names[i])\n",
    "        \n",
    "    axes[0].set(ylabel='Magnitude', xlim=[0,4])\n",
    "        \n",
    "\n",
    "    handlers = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(*handlers,loc='upper center', bbox_to_anchor=(0.5, 1.1),ncols=5)\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_pdf(data,figname):\n",
    "\n",
    "    ## interpolation\n",
    "    interp_test_nonan = []\n",
    "    for i in range(3):\n",
    "        mask = ~np.isnan(data[2][0,...,i])\n",
    "        interp_test_nonan.append(data[2][...,i][:,mask])\n",
    "\n",
    "    ## plot\n",
    "    fig,axes = plt.subplots(1,3,figsize=(7,3))\n",
    "    for i,var in zip(range(3),['$u_1^\\prime$','$u_2^\\prime$','$p^\\prime$']):\n",
    "        # true\n",
    "        counts_true,bins_true = np.histogram(data[0][...,i].flatten()-np.mean(data[0][...,i].flatten()), density=True, bins='auto')\n",
    "        axes[i].stairs(counts_true,bins_true,label='True',linewidth=3, color='#808080',alpha=0.5)\n",
    "        # interpolation\n",
    "        counts,bins = np.histogram(interp_test_nonan[i].flatten()-np.mean(interp_test_nonan[i].flatten()), density=True, bins='auto')\n",
    "        axes[i].stairs(counts,bins,label='Interp.',color='k',linestyle='--')\n",
    "        \n",
    "        # prediction\n",
    "        for j,lossfn in enumerate(['$\\mathcal{L}^c$','$\\mathcal{L}^s$','$\\mathcal{L}^m$']):\n",
    "            counts,bins = np.histogram(data[j+3][...,i].flatten()-np.mean(data[j+3][...,i].flatten()), density=True, bins='auto')\n",
    "            axes[i].stairs(counts,bins,label=lossfn)\n",
    "        axes[i].set(xlabel=var)\n",
    "    axes[0].set_ylabel('Probability density')\n",
    "\n",
    "    handlers = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(*handlers,loc='upper center', bbox_to_anchor=(0.5, 1.1),ncols=5)\n",
    "\n",
    "    if figname:\n",
    "        plt.savefig('./figs/'+figname,bbox_inches='tight')\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_time = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNR20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes about 10 minutes to run the cell\n",
    "results_20, datainfo, observed_20 = get_single_case_predictions(run_snr20_classic)\n",
    "results_20 = list(results_20)\n",
    "for run in [run_snr20_3, run_snr20_mean3]:\n",
    "    result = get_single_case_predictions(run, predict_only=True)\n",
    "    results_20.append(result)\n",
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_snapshots(results_20, '2dtriangle_noisy20_snapshots'+str(plt_time), plt_time)\n",
    "make_image_snapshots_vorticity(results_20, datainfo, '2dtriangle_noisy20_snapshots'+str(plt_time), plt_time)\n",
    "# make_image_freq(results_20, '2dtriangle_noisy20_freq', datainfo.dt)\n",
    "# make_image_pdf(results_20, '2dtriangle_noisy20_pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_10, datainfo, observed_10 = get_single_case_predictions(run_snr10_classic)\n",
    "results_10 = list(results_10)\n",
    "for run in [run_snr10_3, run_snr10_mean3]:\n",
    "    result = get_single_case_predictions(run, predict_only=True)\n",
    "    results_10.append(result)\n",
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_snapshots(results_10, '2dtriangle_noisy10_snapshots'+str(plt_time), plt_time)\n",
    "make_image_snapshots_vorticity(results_10, datainfo, '2dtriangle_noisy10_snapshots'+str(plt_time), plt_time)\n",
    "make_image_freq(results_10, '2dtriangle_noisy10_freq', datainfo.dt)\n",
    "make_image_pdf(results_10, '2dtriangle_noisy10_pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SNR5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_5, datainfo, observed_5 = get_single_case_predictions(run_snr5_classic)\n",
    "results_5 = list(results_5)\n",
    "for run in [run_snr5_3, run_snr5_mean3]:\n",
    "    result = get_single_case_predictions(run, predict_only=True)\n",
    "    results_5.append(result)\n",
    "print('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_image_snapshots(results_5, '2dtriangle_noisy5_snapshots'+str(plt_time), plt_time)\n",
    "make_image_snapshots_vorticity(results_5, datainfo, '2dtriangle_noisy5_snapshots'+str(plt_time), plt_time)\n",
    "make_image_freq(results_5, '2dtriangle_noisy5_freq', datainfo.dt)\n",
    "make_image_pdf(results_5, '2dtriangle_noisy5_pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(3.5,2))\n",
    "plt.imshow(results_20[0][0,...,2].T, alpha=0.3)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.spy(observed_20[0,...,2].T, color='r', marker='s', markersize=2, alpha=0.6)\n",
    "plt.spy(observed_20[0,...,1].T, color='k', marker='s', markersize=2)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "fig.savefig('./figs/2dtriangle_noisy_sensor_location')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
