{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import h5py\n",
    "import yaml\n",
    "import jax\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('../flowrec/utils/a4.mplstyle')\n",
    "\n",
    "from ml_collections import config_dict\n",
    "from pathlib import Path\n",
    "\n",
    "import flowrec.data as data_utils\n",
    "import flowrec.physics_and_derivatives as derivatives\n",
    "import flowrec.training_and_states as state_utils\n",
    "from flowrec import losses\n",
    "from flowrec.utils import simulation,my_discrete_cmap\n",
    "from flowrec.utils.py_helper import slice_from_tuple\n",
    "from flowrec.utils.system import set_gpu\n",
    "set_gpu(0,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get summary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_base = 132\n",
    "triangle_base_coords = [49,80]\n",
    "data_dir = Path('../local_data/re100/')\n",
    "(ux,uy,pp) = simulation.read_data_2dtriangle(data_dir,x_base)\n",
    "x = np.stack([ux,uy,pp],axis=0)\n",
    "slice_to_keep = ((None,), (None,), (None,250,None), (None,))\n",
    "s = slice_from_tuple(slice_to_keep)\n",
    "x = x[s]\n",
    "[x_train,_,_], _ = data_utils.data_partition(x,\n",
    "    1,\n",
    "    (600,100,100),\n",
    "    REMOVE_MEAN=False,\n",
    "    SHUFFLE=False\n",
    ") # Do not shuffle, do not remove mean for training with physics informed loss\n",
    "[ux_train,uy_train,pp_train] = np.squeeze(np.split(x_train,3,axis=0))\n",
    "pb_train = simulation.take_measurement_base(pp_train,ly=triangle_base_coords,centrex=0)\n",
    "pb_train = np.reshape(pb_train,(600,-1))\n",
    "datainfo = data_utils.DataMetadata(\n",
    "    re = 100.0,\n",
    "    discretisation=[0.125,12./512.,4./128.],\n",
    "    axis_index=[0,1,2],\n",
    "    problem_2d=True\n",
    ").to_named_tuple()\n",
    "rng = jax.random.PRNGKey(10)\n",
    "u_train = np.stack((ux_train,uy_train,pp_train),axis=-1)\n",
    "\n",
    "result_dir = Path('../testresult')\n",
    "def create_summary_file(result_dir: Path):\n",
    "    if Path(result_dir,'summary.h5').exists():\n",
    "        raise ValueError('summary file already exist')\n",
    "    \n",
    "    folder_list = [_d for _d in result_dir.iterdir() if _d.is_dir()]\n",
    "    counter = 0\n",
    "    ## for each run in the sweep, get summary\n",
    "    summary_loss = []\n",
    "    summary_name = []\n",
    "\n",
    "    for d in folder_list:\n",
    "        summary_name.append(str(d.name))\n",
    "        counter = counter + 1\n",
    "        print(counter, d.name)\n",
    "\n",
    "        with open(Path(d,'config.yml'),'r') as f:\n",
    "            cfg = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "        \n",
    "        take_observation, _ = cfg.case.observe(\n",
    "            cfg.data_config, \n",
    "            example_pred_snapshot=u_train[0,...],\n",
    "            example_pin_snapshot=pb_train[0,...],\n",
    "        )\n",
    "        _, train_minmax = take_observation(u_train, init=True) # observed_train is normalised if data_config.normalise is True\n",
    "        state = state_utils.restore_trainingstate(d,'state')\n",
    "        prep_data, make_model = cfg.case.select_model(datacfg=cfg.data_config, mdlcfg=cfg.model_config, traincfg=cfg.train_config)\n",
    "        mdl = make_model(cfg.model_config)\n",
    "\n",
    "        if cfg.data_config.normalise:\n",
    "            [pb_train_normalised], _ = data_utils.normalise(pb_train,range=[train_minmax[-1]])\n",
    "            pb_train_batch = np.array_split(pb_train_normalised,2,0)\n",
    "        else:\n",
    "            pb_train_batch = np.array_split(pb_train,2,0)\n",
    "        \n",
    "        pred_train = []\n",
    "        for inn in pb_train_batch:\n",
    "            pred_train.append(mdl.apply(state.params,rng,inn,TRAINING=False))\n",
    "        pred_train = np.concatenate(pred_train)\n",
    "        if cfg.data_config.normalise: # the input to the network is normalised is normalise=True\n",
    "            pred_train = data_utils.unnormalise_group(pred_train, train_minmax, axis_data=-1, axis_range=0)\n",
    "        observed_pred = take_observation(pred_train)\n",
    "        observed_train = take_observation(u_train)\n",
    "\n",
    "        loss = np.array([\n",
    "            losses.relative_error(pred_train, u_train),\n",
    "            losses.divergence(pred_train[...,:-1], datainfo),\n",
    "            losses.momentum_loss(pred_train, datainfo),\n",
    "            losses.mse(observed_pred, observed_train),\n",
    "        ])\n",
    "        summary_loss.append(loss)\n",
    "        print(loss[0])\n",
    "    \n",
    "    summary_loss = np.array(summary_loss)\n",
    "    with h5py.File(Path(result_dir,'summary.h5'),'w') as hf:\n",
    "        hf.create_dataset('runs_name',data=list(summary_name),dtype=h5py.string_dtype(encoding='utf-8'))\n",
    "        hf.create_dataset('runs_loss',data=summary_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_summary_file(Path(\"../local_results/2dtriangle/sweep_loss_classic_fc2branch/\"))\n",
    "# create_summary_file(Path(\"../local_results/2dtriangle/sweep_loss_3_fc2branch/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load runs summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_dir_3 = Path('../local_results/2dtriangle/sweep_loss_3_fc2branch')\n",
    "with h5py.File(Path(sweep_dir_3,'summary.h5'),'r') as hf:\n",
    "    print(hf.keys())\n",
    "    name_3 = np.array(hf.get('runs_name')).astype('unicode')\n",
    "    loss_3 = np.array(hf.get('runs_loss_train')) # [rel_l2, div, momemtum, sensors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_dir_classic = Path('../local_results/2dtriangle/sweep_loss_classic_fc2branch')\n",
    "with h5py.File(Path(sweep_dir_classic,'summary.h5'),'r') as hf:\n",
    "    name_classic = np.array(hf.get('runs_name')).astype('unicode')\n",
    "    loss_classic = np.array(hf.get('runs_loss_train')) # [rel_l2, div, momemtum, sensors]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_3_p = np.concatenate((loss_3[:,[0]],np.sum(loss_3[:,[1,2]],axis=-1,keepdims=True),loss_3[:,[-1]]),axis=-1)\n",
    "loss_classic_p = np.concatenate((loss_classic[:,[0]],np.sum(loss_classic[:,[1,2]],axis=-1,keepdims=True),loss_classic[:,[-1]]),axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean3 = np.mean(loss_3_p,axis=0)\n",
    "std3 = np.std(loss_3_p,axis=0)\n",
    "meanclassic = np.mean(loss_classic_p,axis=0)\n",
    "stdclassic = np.std(loss_classic_p,axis=0)\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=(5,2))\n",
    "p3 = np.array([1.,2.,3.])-0.2\n",
    "pc = np.array([1.,2.,3.])+0.2\n",
    "ax.set(yscale='log',xticks=[1,2,3],xticklabels=['$\\epsilon$','$\\mathcal{L}_p$','$\\mathcal{L}_o$'])\n",
    "violin1 = ax.violinplot(loss_3_p,positions=p3,widths=0.2,showextrema=False,showmeans=True)\n",
    "violin2 = ax.violinplot(loss_classic_p,positions=pc,widths=0.2,showextrema=False,showmeans=True)\n",
    "ax.grid(axis='y')\n",
    "\n",
    "labels = []\n",
    "color = violin1[\"bodies\"][0].get_facecolor().flatten()\n",
    "labels.append((matplotlib.patches.Patch(color=color), '$\\mathcal{L}^s$'))\n",
    "color = violin2[\"bodies\"][0].get_facecolor().flatten()\n",
    "labels.append((matplotlib.patches.Patch(color=color), '$\\mathcal{L}^c$'))\n",
    "\n",
    "for i in range(3):\n",
    "    ax.text(p3[i]-0.09,mean3[i]-mean3[i]/1.2,\"{:.3f}\".format(std3[i]))\n",
    "    ax.text(pc[i]-0.09,mean3[i]-mean3[i]/1.2,\"{:.3f}\".format(stdclassic[i]))\n",
    "\n",
    "plt.legend(*zip(*labels),loc=3)\n",
    "# plt.savefig('./figs/clean_compare_lossfn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.count_nonzero(loss_3[:,0]<0.1)/len(name_3))\n",
    "print(np.count_nonzero(loss_classic[:,0]<0.1)/len(name_classic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "# plt.grid(axis='y')\n",
    "plt.bar([0.9,1.9,2.9],mean3,width=0.2,yerr=std3,label='$\\mathcal{L}^s$')\n",
    "plt.bar([1.1,2.1,3.1],meanclassic,width=0.2,yerr=stdclassic,label='$\\mathcal{L}^c$')\n",
    "plt.yscale('log')\n",
    "# plt.ylim([0,0.6])\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.xticks(ticks=[1,2,3],labels=['$\\epsilon$', '$\\mathcal{L}_p$','$\\mathcal{L}_o$'])\n",
    "# plt.savefig('./figs/clean_compare_lossfn')\n",
    "plt.show()\n",
    "print(std3, stdclassic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_3_total = np.sum(loss_3[:,1:],axis=-1)\n",
    "l_classic_total = np.sum(loss_classic[:,1:],axis=-1)\n",
    "l_3_physics = loss_3_p[:,1]\n",
    "l_classic_physics = loss_classic_p[:,1]\n",
    "\n",
    "# correlation coefficients\n",
    "corr3 = np.corrcoef(l_3_total,loss_3_p[:,0])\n",
    "corrclassic = np.corrcoef(l_classic_total,loss_classic_p[:,0])\n",
    "\n",
    "\n",
    "idx_c = np.argmin(l_classic_total)\n",
    "print(name_classic[idx_c], l_classic_total[idx_c], loss_classic[:,0][idx_c])\n",
    "idx_c = np.argmin(l_3_total)\n",
    "print(name_3[idx_c], l_3_total[idx_c], loss_3[:,0][idx_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.scatter(l_3_total,loss_3[:,0],color=my_discrete_cmap(0),label='$\\mathcal{L}^s$, '+f'{corr3[0,1]:.3f}',marker='+',alpha=0.8)\n",
    "ax.scatter(l_classic_total,loss_classic[:,0],color=my_discrete_cmap(2),label='$\\mathcal{L}^c$, '+ f'{corrclassic[0,1]:.3f}',marker='*',alpha=0.8)\n",
    "\n",
    "# line of best fit\n",
    "plt.plot(np.unique(l_classic_total), np.poly1d(np.polyfit(l_classic_total, loss_classic[:,0], 1))(np.unique(l_classic_total)),color=my_discrete_cmap(2),alpha=0.5)\n",
    "plt.plot(np.unique(l_3_total), np.poly1d(np.polyfit(l_3_total, loss_3[:,0], 1))(np.unique(l_3_total)),color=my_discrete_cmap(0),alpha=0.5)\n",
    "\n",
    "ax.set(yscale='log',xscale='log')\n",
    "ax.set(xlim=[0,1],ylim=[0,1])\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xlabel('total loss')\n",
    "ax.set_ylabel('rel_l2')\n",
    "ax.hlines(1.0,xmin=0.001,xmax=1,color='k',linestyles='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify results\n",
    "Verify that the 'good' ones are good and 'bad' ones are bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compare robustness with respect to the range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.backends.backend_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_results_dir = Path('../local_results/2dtriangle/sweep_loss_3_fc2branch/leafy-sweep-7')\n",
    "# with open(Path(temp_results_dir,'config.yml'),'r') as f:\n",
    "#     cfg = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "#     cfg.data_config.data_dir = '.'+cfg.data_config.data_dir\n",
    "\n",
    "# x_base = 132\n",
    "# triangle_base_coords = [49,80]\n",
    "# (ux,uy,pp) = simulation.read_data_2dtriangle(cfg.data_config.data_dir,x_base)\n",
    "# x = np.stack([ux,uy,pp],axis=0)\n",
    "# # remove parts where uz is not zero\n",
    "# s = slice_from_tuple(cfg.data_config.slice_to_keep)\n",
    "# x = x[s]\n",
    "\n",
    "# [x_train,x_val,x_test], _ = data_utils.data_partition(x,\n",
    "#     1,\n",
    "#     cfg.data_config.train_test_split,\n",
    "#     REMOVE_MEAN=cfg.data_config.remove_mean,\n",
    "#     SHUFFLE=cfg.data_config.shuffle\n",
    "# ) # Do not shuffle, do not remove mean for training with physics informed loss\n",
    "\n",
    "# [ux_train,uy_train,pp_train] = np.squeeze(np.split(x_train,3,axis=0))\n",
    "# [ux_val,uy_val,pp_val] = np.squeeze(np.split(x_val,3,axis=0))\n",
    "# [ux_test,uy_test,pp_test] = np.squeeze(np.split(x_test,3,axis=0))\n",
    "\n",
    "# datainfo = data_utils.DataMetadata(\n",
    "#     re = cfg.data_config.re,\n",
    "#     discretisation=[cfg.data_config.dt,cfg.data_config.dx,cfg.data_config.dy],\n",
    "#     axis_index=[0,1,2],\n",
    "#     problem_2d=True\n",
    "# ).to_named_tuple()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = matplotlib.backends.backend_pdf.PdfPages(\"test3.pdf\")\n",
    "\n",
    "# for i in range(len(name_3)):\n",
    "#     sweep_name = name_3[i]\n",
    "#     results_dir = Path(sweep_dir_3,sweep_name)\n",
    "#     with open(Path(results_dir,'config.yml'),'r') as f:\n",
    "#         cfg = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "\n",
    "#     ## normalise\n",
    "#     if cfg.data_config.normalise:\n",
    "#         # [ux_train_normal,uy_train_normal,pp_train_normal], train_minmax = data_utils.normalise(ux_train,uy_train,pp_train)\n",
    "#         [ux_val_normal,uy_val_normal,pp_val_normal], val_minmax = data_utils.normalise(ux_val,uy_val,pp_val)\n",
    "#         # [ux_test_normal,uy_test_normal,pp_test_normal], test_minmax = data_utils.normalise(ux_test,uy_test,pp_test)\n",
    "#         ## take input\n",
    "#         # pb_train = simulation.take_measurement_base(pp_train_normal,ly=triangle_base_coords,centrex=0)\n",
    "#         pb_val = simulation.take_measurement_base(pp_val_normal,ly=triangle_base_coords,centrex=0)\n",
    "#         # pb_test = simulation.take_measurement_base(pp_test_normal,ly=triangle_base_coords,centrex=0)\n",
    "#     else:\n",
    "#         ## take input\n",
    "#         # pb_train = simulation.take_measurement_base(pp_train,ly=triangle_base_coords,centrex=0)\n",
    "#         pb_val = simulation.take_measurement_base(pp_val,ly=triangle_base_coords,centrex=0)\n",
    "#         # pb_test = simulation.take_measurement_base(pp_test,ly=triangle_base_coords,centrex=0)\n",
    "\n",
    "#     # pb_train = np.reshape(pb_train,(cfg.data_config.train_test_split[0],-1))\n",
    "#     pb_val = np.reshape(pb_val,(cfg.data_config.train_test_split[1],-1))\n",
    "#     # pb_test = np.reshape(pb_test,(cfg.data_config.train_test_split[2],-1))\n",
    "\n",
    "#     # u_train = np.stack((ux_train,uy_train,pp_train),axis=-1)\n",
    "#     u_val = np.stack((ux_val,uy_val,pp_val),axis=-1)\n",
    "#     # u_test = np.stack((ux_test,uy_test,pp_test),axis=-1)\n",
    "\n",
    "\n",
    "#     state = state_utils.restore_trainingstate(results_dir,'state')\n",
    "#     _, make_model = cfg.case.select_model(datacfg=cfg.data_config, mdlcfg=cfg.model_config, traincfg=cfg.train_config)\n",
    "#     mdl = make_model(cfg.model_config)\n",
    "#     rng = jax.random.PRNGKey(10)\n",
    "#     pred_val = mdl.apply(state.params,rng,pb_val,TRAINING=False)\n",
    "#     if cfg.data_config.normalise:\n",
    "#         pred_val = data_utils.unnormalise_group(pred_val, val_minmax, axis_data=-1, axis_range=0)\n",
    "    \n",
    "\n",
    "#     take_observation, _ = cfg.case.observe(cfg.data_config, example_pred_snapshot=u_val[0,...],example_pin_snapshot=pb_val[0,...])\n",
    "    \n",
    "#     # l_s_val = losses.mse(take_observation(pred_val),take_observation(u_val))\n",
    "#     # l_d_val = np.mean(derivatives.div_field(pred_val[...,:-1],datainfo)**2)\n",
    "#     # l_m_val = np.mean(derivatives.momentum_residual_field(pred_val,datainfo)**2)\n",
    "#     l_mse_clean = losses.relative_error(pred_val,u_val) \n",
    "\n",
    "#     fig,ax = plt.subplots(1,3,figsize=(8,3))\n",
    "#     im1 = ax[0].imshow(pred_val[20,:,:,0],vmin=-0.6,vmax=1.8)\n",
    "#     plt.colorbar(im1,ax=ax[0])\n",
    "#     im2 = ax[1].imshow(pred_val[20,:,:,1],vmin=-1,vmax=1.5)\n",
    "#     plt.colorbar(im2,ax=ax[1])\n",
    "#     im3 = ax[2].imshow(pred_val[20,:,:,2],vmin=-1.5,vmax=0.25)\n",
    "#     plt.colorbar(im3,ax=ax[2])\n",
    "#     fig.suptitle(str(sweep_name))\n",
    "\n",
    "\n",
    "# fig_nums = plt.get_fignums()  \n",
    "# figs = [plt.figure(n) for n in fig_nums]\n",
    "# for fig in figs: ## will open an empty extra figure :(\n",
    "#     pdf.savefig(fig)\n",
    "# pdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(zip(name_3,loss_3[:,0])))\n",
    "print(list(zip(name_classic,loss_classic[:,0])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0c4f52e826ba972db993c88e86bb40a803fc6f9fbf0ce58ed5ef88c4855dac4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
