{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from flowrec.utils.system import set_gpu\n",
    "set_gpu(0,0.7)\n",
    "import jax\n",
    "import jax.numpy\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('./flowrec/utils/ppt.mplstyle')\n",
    "from pathlib import Path\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowrec.utils.simulation import read_data_volvo\n",
    "import flowrec.physics_and_derivatives as derivatives\n",
    "import flowrec.losses as losses\n",
    "from flowrec.data import DataMetadata, normalise, unnormalise_group\n",
    "from flowrec.utils.py_helper import slice_from_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_p, d, re, density = read_data_volvo('./local_data/volvorig/u166', nondimensional=True)\n",
    "u_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datainfo = DataMetadata(\n",
    "    re=re,\n",
    "    discretisation=d,\n",
    "    axis_index=[0,1,2,3],\n",
    "    problem_2d=False,\n",
    ").to_named_tuple()\n",
    "datainfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slice: Load data and measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 200\n",
    "u_train = u_p[:n_train,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_plane = 10\n",
    "y_skip = 1\n",
    "inns_loc = np.s_[:,0:1,::y_skip,:,-1]\n",
    "inns = u_train[inns_loc]\n",
    "inns_shape = inns.shape\n",
    "print(inns_shape)\n",
    "inns = np.squeeze(inns)\n",
    "y_loc = np.s_[:,:,:,z_plane,:-1]\n",
    "y = u_train[y_loc]\n",
    "print(inns.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measured = np.empty_like(u_train)\n",
    "measured[inns_loc] = inns.reshape(inns_shape)\n",
    "measured[y_loc] = y\n",
    "print(measured.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_t = 10\n",
    "vmin = u_train[plt_t,...,-1].min()\n",
    "vmax = u_train[plt_t,...,-1].max()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,3))\n",
    "axes[0].set_title(f'The measured plane z={z_plane}')\n",
    "axes[0].imshow(u_train[plt_t,:,:,z_plane,-1].T, vmin=vmin, vmax=vmax)\n",
    "axes[0].set(xlabel='x', ylabel='y')\n",
    "axes[1].imshow(u_train[plt_t,:,0,:,-1].T, vmin=vmin, vmax=vmax)\n",
    "axes[1].spy(measured[plt_t,:,0,:,-1].T,alpha=0.5)\n",
    "axes[1].set(xlabel='x', ylabel='z')\n",
    "imyz = axes[2].imshow(u_train[plt_t,0,:,:,-1].T, vmin=vmin, vmax=vmax)\n",
    "axes[2].spy(measured[plt_t,0,:,:,-1].T,alpha=0.3)\n",
    "axes[2].set(xlabel='y', ylabel='z')\n",
    "plt.colorbar(imyz)\n",
    "fig.suptitle('True flow pressure')\n",
    "plt.show()\n",
    "vmin = u_train[plt_t,...,0].min()\n",
    "vmax = u_train[plt_t,...,0].max()\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10,3))\n",
    "axes[0].set_title(f'The measured plane z={z_plane}')\n",
    "axes[0].imshow(u_train[plt_t,:,:,z_plane,0].T, vmin=vmin, vmax=vmax)\n",
    "axes[0].set(xlabel='x', ylabel='y')\n",
    "axes[1].imshow(u_train[plt_t,:,0,:,0].T, vmin=vmin, vmax=vmax)\n",
    "axes[1].spy(measured[plt_t,:,0,:,0].T,alpha=0.5)\n",
    "axes[1].set(xlabel='x', ylabel='z')\n",
    "imyz = axes[2].imshow(u_train[plt_t,0,:,:,0].T, vmin=vmin, vmax=vmax)\n",
    "axes[2].spy(measured[plt_t,0,:,:,0].T,alpha=0.3)\n",
    "axes[2].set(xlabel='y', ylabel='z')\n",
    "plt.colorbar(imyz)\n",
    "fig.suptitle('True flow u1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_ref = losses.divergence(u_train[...,:-1],datainfo)\n",
    "lm_ref = losses.momentum_loss(u_train, datainfo)\n",
    "print('Div and momentum loss ',ld_ref, lm_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Poisson equation for pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2pdx1 = derivatives.derivative2(u_train[...,-1], h=datainfo.dx, axis=datainfo.axx)\n",
    "d2pdx2 = derivatives.derivative2(u_train[...,-1], h=datainfo.dy, axis=datainfo.axy)\n",
    "d2pdx3 = derivatives.derivative2(u_train[...,-1], h=datainfo.dz, axis=datainfo.axz)\n",
    "dp2 = d2pdx1+d2pdx2+d2pdx3\n",
    "dp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_over_u = jax.vmap(derivatives.derivative1, (4,None,None),4)\n",
    "_duidx1 = d1_over_u(u_train[...,:-1],datainfo.dx,datainfo.axx) # [t...i](d_dx1)\n",
    "_duidx2 = d1_over_u(u_train[...,:-1],datainfo.dy,datainfo.axy) # [t...i](d_dx2)\n",
    "_duidx3 = d1_over_u(u_train[...,:-1],datainfo.dz,datainfo.axz) # [t...i](d_dx3)\n",
    "duidxj = jnp.stack((_duidx1, _duidx2, _duidx3), axis=5) # [txyzij]\n",
    "uj_duidxj = jnp.einsum('...j, ...ij -> ...i', u_train[...,:-1], duidxj)\n",
    "print(uj_duidxj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udu1dx_dx1 = derivatives.derivative2(uj_duidxj[...,0], h=datainfo.dx, axis=datainfo.axx)\n",
    "udu2dx_dx2 = derivatives.derivative2(uj_duidxj[...,1], h=datainfo.dy, axis=datainfo.axy)\n",
    "udu3dx_dx3 = derivatives.derivative2(uj_duidxj[...,2], h=datainfo.dz, axis=datainfo.axz)\n",
    "div_ududx = udu1dx_dx1+udu2dx_dx2+udu3dx_dx3\n",
    "print(div_ududx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses.mse(div_ududx + dp2))\n",
    "print(div_ududx + dp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flowrec.training_and_states as train\n",
    "import optax\n",
    "from train_config.train_options import optimizer as optimiser_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [inns_norm], inns_range = normalise(inns, range=[[inns[:,:,z_plane].min(),inns[:,:,z_plane].max()]])\n",
    "[inns_norm], inns_range = normalise(inns)\n",
    "y_list, y_range = normalise(y[...,0],y[...,1],y[...,2])\n",
    "y_norm = np.stack(y_list, axis=-1)\n",
    "print(y_norm.shape)\n",
    "refvelocity_norm, _ = normalise(u_train[...,0],u_train[...,1],u_train[...,2], range=y_range)\n",
    "refvelocity_norm = np.stack(refvelocity_norm, axis=-1)\n",
    "all_range = y_range.copy()\n",
    "all_range.extend(inns_range)\n",
    "print(all_range)\n",
    "u_train_norm,_ = normalise(*[np.squeeze(_u) for _u in np.split(u_train,4, axis=-1)], range=all_range)\n",
    "u_train_norm = np.stack(u_train_norm, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train on slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowrec.models.feedforward import Model as FFMDL\n",
    "from flowrec.models.feedforward import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1 = FFMDL(\n",
    "    layers=[128,256, 512, 1024,y[0,...].size],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params1 = mdl1.init(key,inns_norm[:10,:,z_plane])\n",
    "scheduler1 = optimiser_options.get_scheduler('exponential_decay', lr)\n",
    "optimiser1 = optax.adamw(learning_rate=scheduler1)\n",
    "opt_state1 = optimiser1.init(params1)\n",
    "state1 = train.TrainingState(params=params1, opt_state=opt_state1)\n",
    "mdl1.apply(state1.params, key, inns_norm[:10,:,z_plane]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = []\n",
    "best_l = jnp.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update1 = train.generate_update_fn(\n",
    "    mdl1.apply,\n",
    "    optimiser1,\n",
    "    losses.loss_mse,\n",
    ")\n",
    "_l, _ = update1(state1, key, inns_norm[:10,:,z_plane], y_norm[:10,...].reshape((10,-1)))\n",
    "print(_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "for i in range(n_epochs):\n",
    "    [key] = jax.random.split(key,1)\n",
    "    l_epoch = []\n",
    "    j = 0\n",
    "    while (j+1)*50 <= n_train :\n",
    "        _l, state1 = update1(state1, key, inns_norm[j*50:(j+1)*50,:,z_plane], y_norm[j*50:(j+1)*50,...].reshape((50,-1)))\n",
    "        j += 1\n",
    "        l_epoch.append(_l)\n",
    "    l = np.mean(l_epoch)\n",
    "    hist1.append(l)\n",
    "    if i % 20 == 0:\n",
    "        print(f'epoch {i}, loss {l}')\n",
    "    if l < best_l:\n",
    "        best_state1 = state1\n",
    "        best_l = l\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.loglog(hist1)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_slice_norm = mdl1.apply(best_state1.params, None, inns_norm[:,:,z_plane]).reshape(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_step = 40\n",
    "vmin = y_norm[::plt_step,...,0].min()\n",
    "vmax = y_norm[::plt_step,...,0].max()\n",
    "fig,axes = plt.subplots(3,5,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Normalized results. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(5):\n",
    "    axes[2,i].plot(inns_norm[i*plt_step,:,z_plane])\n",
    "    axes[2,i].set(xlabel=f'y at z={z_plane}, t={i*plt_step}', ylim=[inns_norm[::plt_step,:,z_plane].min(), inns_norm[::plt_step,:,z_plane].max()])\n",
    "    axes[0,i].imshow(y_norm[i*plt_step,...,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    axes[1,i].imshow(pred_slice_norm[i*plt_step,...,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_slice = unnormalise_group(pred_slice_norm, data_range=y_range, axis_data=-1, axis_range=0)\n",
    "vmin = y[::plt_step,...,0].min()\n",
    "vmax = y[::plt_step,...,0].max()\n",
    "fig,axes = plt.subplots(3,5,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Unnormalized results. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(5):\n",
    "    axes[2,i].plot(inns[i*plt_step,:,z_plane])\n",
    "    axes[2,i].set(xlabel=f'y at z={z_plane}, t={i*plt_step}', ylim=[inns[::plt_step,:,z_plane].min(), inns[::plt_step,:,z_plane].max()])\n",
    "    imref = axes[0,i].imshow(y[i*plt_step,...,0].T, vmin=vmin, vmax=vmax)\n",
    "    # plt.colorbar(imref)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    impred = axes[1,i].imshow(pred_slice[i*plt_step,...,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "    # plt.colorbar(impred)\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl1_predict_overz = jax.vmap(mdl1.predict, (None,2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predv1_norm = mdl1_predict_overz(best_state1.params, inns_norm).reshape((*y.shape,-1))\n",
    "predv1_norm = np.einsum('txyuz -> txyzu', predv1_norm)\n",
    "print(f'volume mse {losses.mse(predv1_norm,refvelocity_norm)}, measured plane {losses.mse(predv1_norm[:,:,:,z_plane,:],refvelocity_norm[:,:,:,z_plane,:])}')\n",
    "for i in range(0,z_plane):\n",
    "    print(losses.mse(predv1_norm[:,:,:,i,:],refvelocity_norm[:,:,:,i,:]))\n",
    "predv1 = unnormalise_group(predv1_norm, data_range=y_range, axis_data=-1, axis_range=0)\n",
    "print(f'Divergence {losses.divergence(predv1,datainfo)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_zgap = 2\n",
    "plt_t = 100\n",
    "vmin = refvelocity_norm[plt_t,...,::plt_zgap,0].min()\n",
    "vmax = refvelocity_norm[plt_t,...,::plt_zgap,0].max()\n",
    "fig,axes = plt.subplots(3,6,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Normalized results. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(6):\n",
    "    axes[2,i].plot(inns_norm[plt_t,:,i*plt_zgap])\n",
    "    axes[2,i].set(xlabel=f'y at z={i*plt_zgap}, t={plt_t}', ylim=[inns_norm[plt_t,:,::plt_zgap].min(), inns_norm[plt_t,:,::plt_zgap].max()])\n",
    "    axes[0,i].imshow(refvelocity_norm[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    axes[1,i].imshow(predv1_norm[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on volume "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct for z locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "\n",
    "v_resize = jax.vmap(jax.tree_util.Partial(jax.image.resize,method='linear'),(-1,None),-1)\n",
    "vv_resize = jax.vmap(v_resize,(0,None),0)\n",
    "[key] = jax.random.split(key,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward3(x,p,z_coord):\n",
    "    l1 = hk.Conv2D(4,3)\n",
    "    l2 = hk.Conv2D(8,3)\n",
    "    l3 = [hk.Linear(n) for n in [50,10]]\n",
    "    l4 = [hk.Linear(n) for n in [100,50,10]]\n",
    "    l5 = [hk.Linear(n) for n in [128,256, 512, 1024, 40*20*3]]\n",
    "\n",
    "    def inner(x1,p1,z_coord):\n",
    "        # print(x1.shape,p1.shape,z_coord.shape)\n",
    "        x1 = l1(x1)\n",
    "        x1 = vv_resize(x1, (20,10))\n",
    "        x1 = jnp.tanh(x1)\n",
    "        x1 = l2(x1)\n",
    "        x1 = vv_resize(x1, (10,5))\n",
    "        x1 = jnp.tanh(x1)\n",
    "        x1 = x1.reshape((-1,10*5*8))\n",
    "        for l in l3:\n",
    "            x1 = l(x1)\n",
    "            x1 = jnp.tanh(x1)\n",
    "        for l in l4:\n",
    "            p1 = l(p1)\n",
    "            p1 = jnp.tanh(p1)\n",
    "        # print(x1.shape,p1.shape,z_coord.shape)\n",
    "        out = jnp.concatenate((x1,p1,z_coord,),axis=1)\n",
    "        # print(out.shape)\n",
    "        for l in l5[:-1]:\n",
    "            out = l(out)\n",
    "            out = jnp.tanh(out)\n",
    "        return l5[-1](out).reshape((-1,40,20,3))\n",
    "    out1 = jax.vmap(inner, (0,0,None),0)(x,p,z_coord.reshape((20,1)))\n",
    "    out1 = jnp.einsum('tzxyu -> txyzu', out1)\n",
    "    return jnp.einsum('utxyz -> txyzu',derivatives.vorticity(out1,datainfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_coord = (jnp.arange(0,20)-10)/10\n",
    "inns3_group = (jnp.einsum('txyzu -> tzxyu', predv1_norm)[:,...], jnp.einsum('tyz -> tzy', inns_norm)[:,...], z_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl3 = hk.transform(forward3)\n",
    "params3 = mdl3.init(key, jnp.einsum('txyzu -> tzxyu', predv1_norm)[:10,...], jnp.einsum('tyz -> tzy', inns_norm)[:10,...], z_coord)\n",
    "mdl3_apply = jax.jit(mdl3.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.tree_map(lambda x: print(x.shape), params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "scheduler3 = optimiser_options.get_scheduler('exponential_decay', lr)\n",
    "optimiser3 = optax.adamw(learning_rate=scheduler3)\n",
    "opt_state3 = optimiser3.init(params3)\n",
    "state3 = train.TrainingState(params=params3, opt_state=opt_state3)\n",
    "mdl3_apply(state3.params, key, jnp.einsum('txyzu -> tzxyu', predv1_norm)[:10,...], jnp.einsum('tyz -> tzy', inns_norm)[:10,...], z_coord).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss3(apply_fn, params, rng, inns_group, y):\n",
    "    pred = apply_fn(params, rng, *inns_group)\n",
    "    # pred = jnp.einsum('tzxyu -> txyzu', pred)\n",
    "    ld = losses.mse(pred[:,:,:,z_plane,:], y) # take data loss before unnormalising\n",
    "    # pred_new = pred.at[:,:,:,z_plane,:].set(y)\n",
    "    ## unnormalise because div(u) is not div(normalised(u))\n",
    "    # pred_new = unnormalise_group(pred_new, data_range=y_range, axis_data=-1, axis_range=0)\n",
    "    ldiv = losses.divergence(pred, datainfo)\n",
    "    return 2*ld + 0*ldiv, {'plane': ld, 'div':ldiv}\n",
    "update3 = train.generate_update_fn(mdl3_apply, optimiser3, loss3, kwargs_value_and_grad={'has_aux':True})\n",
    "l,_ = update3(state3, key, inns3_group, y)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist3 = {'loss':[], 'div':[], 'plane':[]}\n",
    "best_l = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "for i in range(n_epochs):\n",
    "    [key] = jax.random.split(key,1)\n",
    "    l_epoch = {'loss':[], 'div':[], 'plane':[]}\n",
    "    j = 0\n",
    "    while (j+1)*50 <= n_train :\n",
    "        _inns3 = (jnp.einsum('txyzu -> tzxyu', predv1)[j*50:(j+1)*50,...], jnp.einsum('tyz -> tzy', inns_norm)[j*50:(j+1)*50,...], z_coord)\n",
    "        (_l, _l_components), state3 = update3(state3, key, _inns3, y[j*50:(j+1)*50,...])\n",
    "        j += 1\n",
    "        l_epoch['loss'].append(float(_l))\n",
    "        for k,a in _l_components.items():\n",
    "            l_epoch[k].append(float(a))\n",
    "    for k,a in l_epoch.items():\n",
    "        hist3[k].append(np.mean(a))\n",
    "    if i % 20 == 0:\n",
    "        print(f'epoch {i}, ', \" \".join(f\"{k}: {a[-1]:.5f}\" for k, a in hist3.items()))\n",
    "    if hist3['loss'][-1] < best_l:\n",
    "        best_state3 = state3\n",
    "        best_l = hist3['loss'][-1]\n",
    "plt.figure(figsize=(5,3))\n",
    "for k in hist3.keys():\n",
    "    plt.semilogy(hist3[k], label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predv3 = mdl3_apply(best_state3.params, None, *inns3_group)\n",
    "# predv3_norm = np.einsum('tzxyu -> txyzu', predv3_norm)\n",
    "print(f'volume mse {losses.relative_error(predv3,u_train[...,:-1])}, measured plane {losses.relative_error(predv3[:,:,:,z_plane,:],u_train[:,:,:,z_plane,:-1])}')\n",
    "for i in range(0,z_plane):\n",
    "    print(losses.relative_error(predv3[:,:,:,i,:],u_train[:,:,:,i,:-1]))\n",
    "# predv3 = unnormalise_group(predv3_norm, data_range=y_range, axis_data=-1, axis_range=0)\n",
    "print(f'Divergence {losses.divergence(predv3,datainfo)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_zgap = 2\n",
    "plt_t = 100\n",
    "vmin = u_train[plt_t,...,::plt_zgap,0].min()\n",
    "vmax = u_train[plt_t,...,::plt_zgap,0].max()\n",
    "fig,axes = plt.subplots(3,6,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Normalized results. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(6):\n",
    "    axes[2,i].plot(inns_norm[plt_t,:,i*plt_zgap])\n",
    "    axes[2,i].set(xlabel=f'y at z={i*plt_zgap}, t={plt_t}', ylim=[inns_norm[plt_t,:,::plt_zgap].min(), inns_norm[plt_t,:,::plt_zgap].max()])\n",
    "    axes[0,i].imshow(u_train[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    axes[1,i].imshow(predv3[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try a different set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward3_1(p,z_coord):\n",
    "    _mlp = MLP(\n",
    "        [128,256, 512, 1024,y[0,...].size],\n",
    "        name='mlp',\n",
    "        w_init=hk.initializers.VarianceScaling(1.0,'fan_avg','uniform'),\n",
    "        activation=jax.nn.tanh\n",
    "        )\n",
    "    # linear_z = hk.Linear(y[0,...].size,with_bias=False,name='linear_correction')\n",
    "    \n",
    "    linear_p = hk.Linear(y[0,...].size,with_bias=False,name='linear_correction_pressure')\n",
    "    linear_z = hk.Linear(20,with_bias=False,name='linear_correction')\n",
    "\n",
    "    # apply to all z\n",
    "    def inner(p1,z1):\n",
    "        # print(p1.shape)\n",
    "        out1 = _mlp(p1, TRAINING=True)\n",
    "        # combine\n",
    "        # correction = linear_z(z1) # correct only with z\n",
    "        \n",
    "        # correct with both pressure and z\n",
    "        correctionz = linear_z(z1)\n",
    "        p2 = jnp.einsum('zi,zi -> zi', p1, correctionz) # zero if z is 0 \n",
    "        correction = linear_p(p2)\n",
    "        return out1 + correction\n",
    "    \n",
    "    # apply over all time\n",
    "    # print(p.shape)\n",
    "    volume_over_time = jax.vmap(inner, (0,None), 0)\n",
    "    return volume_over_time(p,z_coord)\n",
    "\n",
    "mdl3_1 = hk.transform(forward3_1)\n",
    "_p = mdl3_1.init(key, jnp.einsum('txz -> tzx', inns_norm[:10,...]), jnp.reshape(z_coord, (20,1)))\n",
    "print(list(_p))\n",
    "# print(mdl3_1.apply(_p,None, jnp.einsum('txz -> tzx', inns_norm[:10,...]), jnp.reshape(z_coord, (20,1))).shape)\n",
    "print(mdl3_1.apply(_p,None, jnp.einsum('txz -> tzx', inns_norm[:10,:,[10]]), jnp.reshape(np.array([0]), (1,1))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split weights\n",
    "for k, layer in best_state1.params.items():\n",
    "    if k in _p.keys():\n",
    "        _p[k].update(layer)\n",
    "        # print(k, layer.keys())\n",
    "print(np.allclose(_p['mlp/~/linear_0']['w'], best_state1.params['mlp/~/linear_0']['w']))\n",
    "params3_trainable, params3_non_trainable = hk.data_structures.partition(\n",
    "    lambda module_name, name, value: 'correction' in module_name or module_name=='mlp/~/linear_4'or module_name=='mlp/~/linear_3',\n",
    "    _p\n",
    ")\n",
    "print(list(params3_trainable), list(params3_non_trainable))\n",
    "print(jax.tree_map(lambda x: print(x.shape),params3_trainable))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "scheduler3_1 = optimiser_options.get_scheduler('exponential_decay', lr)\n",
    "optimiser3_1 = optax.adamw(learning_rate=scheduler3_1)\n",
    "opt_state3_1 = optimiser3_1.init(params3_trainable)\n",
    "state3_1 = train.TrainingState(params=params3_trainable, opt_state=opt_state3_1)\n",
    "mdl3_1.apply(\n",
    "    hk.data_structures.merge(state3_1.params,params3_non_trainable),\n",
    "    key, \n",
    "    jnp.einsum('txz -> tzx', inns_norm[:10,...]), \n",
    "    jnp.reshape(z_coord, (20,1))\n",
    ").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "inns3_1_group = (\n",
    "    jnp.einsum('txz -> tzx', inns_norm[:,...]), \n",
    "    jnp.reshape(z_coord, (20,1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_error(pred,true):\n",
    "    err = jnp.sqrt(\n",
    "        jnp.sum((pred-true)**2)\n",
    "        / jnp.sum(true**2)\n",
    "    )\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss3_1(apply_fn, trainable, rng, inns_group, y, non_trainable):\n",
    "    # merge all params\n",
    "    params = hk.data_structures.merge(trainable, non_trainable)\n",
    "    \n",
    "    pred = apply_fn(params, rng, *inns_group)\n",
    "    pred = jnp.reshape(pred, (-1,20,)+y.shape[1:])\n",
    "    pred = jnp.einsum('tzxyu -> txyzu', pred)\n",
    "    ld = relative_error(pred[:,:,:,z_plane,:], y) # take data loss before unnormalising\n",
    "    pred_new = pred.at[:,:,:,z_plane,:].set(y)\n",
    "    ## unnormalise because div(u) is not div(normalised(u))\n",
    "    pred_new = unnormalise_group(pred_new, data_range=y_range, axis_data=-1, axis_range=0)\n",
    "    ldiv = losses.divergence(pred_new, datainfo)\n",
    "    return 10*ld + ldiv, {'plane': ld, 'div':ldiv}\n",
    "\n",
    "\n",
    "update3_1 = train.generate_update_fn(\n",
    "    mdl3_1.apply,\n",
    "    optimiser3_1,\n",
    "    loss3_1,\n",
    "    kwargs_loss={'non_trainable': params3_non_trainable},\n",
    "    kwargs_value_and_grad={'has_aux':True}\n",
    ")\n",
    "l,_ = update3_1(state3_1, key, inns3_1_group, y_norm)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist3_1 = {'loss':[], 'div':[], 'plane':[]}\n",
    "best_l = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "batch = 100\n",
    "for i in range(n_epochs):\n",
    "    [key] = jax.random.split(key,1)\n",
    "    l_epoch = {'loss':[], 'div':[], 'plane':[]}\n",
    "    j = 0\n",
    "    while (j+1)*batch <= n_train :\n",
    "        _inns3_1 = (\n",
    "            jnp.einsum('txz -> tzx', inns_norm[j*batch:(j+1)*batch,...]), \n",
    "            jnp.reshape(z_coord, (20,1))\n",
    "        )\n",
    "        (_l, _l_components), state3_1 = update3_1(state3_1, key, _inns3_1, y_norm[j*batch:(j+1)*batch,...])\n",
    "        j += 1\n",
    "        l_epoch['loss'].append(float(_l))\n",
    "        for k,a in _l_components.items():\n",
    "            l_epoch[k].append(float(a))\n",
    "    for k,a in l_epoch.items():\n",
    "        hist3_1[k].append(np.mean(a))\n",
    "    if i % 20 == 0:\n",
    "        print(f'epoch {i}, ', \" \".join(f\"{k}: {a[-1]:.5f}\" for k, a in hist3_1.items()))\n",
    "    if hist3_1['loss'][-1] < best_l:\n",
    "        best_state3_1 = state3_1\n",
    "        best_l = hist3_1['loss'][-1]\n",
    "plt.figure(figsize=(5,3))\n",
    "for k in hist3_1.keys():\n",
    "    plt.semilogy(hist3_1[k], label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predv3_1_norm = mdl3_1.apply(\n",
    "    hk.data_structures.merge(best_state3_1.params,params3_non_trainable),\n",
    "    None, \n",
    "    *inns3_1_group).reshape((-1,20,)+y.shape[1:])\n",
    "predv3_1_norm = np.einsum('tzxyu -> txyzu', predv3_1_norm)\n",
    "print(f'volume relative error {losses.relative_error(predv3_1_norm,refvelocity_norm)}, measured plane {losses.relative_error(predv3_1_norm[:,:,:,z_plane,:],refvelocity_norm[:,:,:,z_plane,:])}')\n",
    "for i in range(0,z_plane+1):\n",
    "    print(losses.relative_error(predv3_1_norm[:,:,:,i,:],refvelocity_norm[:,:,:,i,:]))\n",
    "predv3_1 = unnormalise_group(predv3_1_norm, data_range=y_range, axis_data=-1, axis_range=0)\n",
    "print(f'Divergence {losses.divergence(predv3_1,datainfo)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_zgap = 2\n",
    "plt_t = 100\n",
    "vmin = refvelocity_norm[plt_t,...,::plt_zgap,0].min()\n",
    "vmax = refvelocity_norm[plt_t,...,::plt_zgap,0].max()\n",
    "fig,axes = plt.subplots(3,6,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Normalized results. mdl1+correction. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(6):\n",
    "    axes[2,i].plot(inns_norm[plt_t,:,i*plt_zgap])\n",
    "    axes[2,i].set(xlabel=f'y at z={i*plt_zgap}, t={plt_t}', ylim=[inns_norm[plt_t,:,::plt_zgap].min(), inns_norm[plt_t,:,::plt_zgap].max()])\n",
    "    axes[0,i].imshow(refvelocity_norm[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    axes[1,i].imshow(predv3_1_norm[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try adding pressure straight away MDL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward2(p,z_coord):\n",
    "    final_size = y[0,...].size\n",
    "    _mlp = MLP(\n",
    "        [128,256,512,1024,final_size],\n",
    "        name='mlp',\n",
    "        w_init=hk.initializers.VarianceScaling(1.0),\n",
    "        activation=jax.nn.tanh\n",
    "    )\n",
    "    linear_z = [hk.Linear(512, with_bias=False), hk.Linear(final_size, with_bias=False)]\n",
    "    linear_reduce = [hk.Linear(n) for n in [512,128]]\n",
    "    linear_p = [hk.Linear(n) for n in [128,512,1024,int(final_size/3)]]\n",
    "\n",
    "    def over_z(p1,z1):\n",
    "        u = _mlp(p1, TRAINING=True)\n",
    "        correction = z1\n",
    "        for l in linear_z:\n",
    "            correction = jax.nn.tanh(correction)\n",
    "            correction = l(correction)\n",
    "        u = u + correction\n",
    "        u_reduce = u \n",
    "        for l in linear_reduce:\n",
    "            u_reduce = jax.nn.tanh(u_reduce)\n",
    "            u_reduce = l(u_reduce)\n",
    "        outp = jnp.concatenate((p1,z1,u_reduce),axis=-1)\n",
    "        for l in linear_p:\n",
    "            outp = jax.nn.tanh(outp)\n",
    "            outp = l(outp)\n",
    "        outu = jnp.reshape(u, (-1,40,20,3))\n",
    "        outp = jnp.reshape(outp, (-1,40,20,1))\n",
    "        return jnp.concatenate((outu,outp), axis=-1)\n",
    "\n",
    "    out = jax.vmap(over_z, (0,None),0)(p,z_coord)\n",
    "    out = jnp.einsum('tzxyu -> txyzu', out)\n",
    "    return out\n",
    "mdl2 = hk.transform(forward2)\n",
    "params2 = mdl2.init(key,  jnp.einsum('txz -> tzx', inns_norm[:10,...]), jnp.reshape(z_coord, (20,1)))\n",
    "print(list(params2))\n",
    "print(mdl2.apply(params2, None, jnp.einsum('txz -> tzx', inns_norm[:10,...]), jnp.reshape(z_coord, (20,1))).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split weights\n",
    "for k, layer in best_state1.params.items():\n",
    "    if k in params2.keys():\n",
    "        params2[k].update(layer)\n",
    "        # print(k, layer.keys())\n",
    "print(np.allclose(params2['mlp/~/linear_0']['w'], best_state1.params['mlp/~/linear_0']['w']))\n",
    "params2_non_trainable, params2_trainable = hk.data_structures.partition(\n",
    "    lambda module_name, name, value: module_name in ['mlp/~/linear_0', 'mlp/~/linear_1', 'mlp/~/linear_2', 'mlp/~/linear_3',],\n",
    "    params2\n",
    ")\n",
    "print(list(params2_trainable))\n",
    "print(list(params2_non_trainable))\n",
    "print(jax.tree_map(lambda x: print(x.shape),params2_trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "scheduler2 = optimiser_options.get_scheduler('exponential_decay', lr)\n",
    "optimiser2 = optax.adamw(learning_rate=scheduler2)\n",
    "opt_state2 = optimiser2.init(params2_trainable)\n",
    "state2 = train.TrainingState(params=params2_trainable, opt_state=opt_state2)\n",
    "print(mdl2.apply(\n",
    "    hk.data_structures.merge(state2.params,params2_non_trainable),\n",
    "    key, \n",
    "    jnp.einsum('txz -> tzx', inns_norm[:10,...]), \n",
    "    jnp.reshape(z_coord, (20,1))\n",
    ").shape)\n",
    "inns2_group = (\n",
    "    jnp.einsum('txz -> tzx', inns_norm[:,...]), \n",
    "    jnp.reshape(z_coord, (20,1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss2(apply_fn, trainable, rng, inns_group, yall, non_trainable):\n",
    "    params = hk.data_structures.merge(trainable, non_trainable)\n",
    "    pred = apply_fn(params, rng, *inns_group)\n",
    "    print(pred.shape)\n",
    "    ld = losses.mse(pred[:,:,:,z_plane,0], yall[:,:,:,z_plane,0]) #+ losses.mse(pred[inns_loc],yall[inns_loc]) # data loss, velocities on the plane and pressure at inlet\n",
    "    pred_new = pred.at[:,:,:,z_plane,:-1].set(yall[:,:,:,z_plane,:-1])\n",
    "    pred_new = pred_new.at[inns_loc].set(yall[inns_loc])\n",
    "    # unnormalise before taking physics loss\n",
    "    pred_new = unnormalise_group(pred_new, data_range=all_range, axis_data=-1, axis_range=0)\n",
    "    ldiv = losses.divergence(pred_new[...,:-1], datainfo)\n",
    "    lmom = losses.momentum_loss(pred_new, datainfo)\n",
    "    # Poisson equation for incompressible flow\n",
    "    # lp = \n",
    "    \n",
    "    return 1000*ld + 200*ldiv + lmom, {'plane': ld, 'div':ldiv, 'momentum':lmom}\n",
    "\n",
    "update2 = train.generate_update_fn(\n",
    "    mdl2.apply,\n",
    "    optimiser2,\n",
    "    loss2,\n",
    "    kwargs_loss={'non_trainable': params2_non_trainable},\n",
    "    kwargs_value_and_grad={'has_aux':True}\n",
    ")\n",
    "l,_ = update2(state2, key, inns2_group, u_train_norm)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predv2_norm = mdl2.apply(\n",
    "    hk.data_structures.merge(params2),\n",
    "    None, \n",
    "    *inns2_group\n",
    ")\n",
    "plt_zgap = 2\n",
    "plt_t = 100\n",
    "vmin = refvelocity_norm[plt_t,...,::plt_zgap,0].min()\n",
    "vmax = refvelocity_norm[plt_t,...,::plt_zgap,0].max()\n",
    "fig,axes = plt.subplots(3,6,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Normalized results. mdl1+correction. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(6):\n",
    "    axes[2,i].plot(inns_norm[plt_t,:,i*plt_zgap])\n",
    "    axes[2,i].set(xlabel=f'y at z={i*plt_zgap}, t={plt_t}', ylim=[inns_norm[plt_t,:,::plt_zgap].min(), inns_norm[plt_t,:,::plt_zgap].max()])\n",
    "    axes[0,i].imshow(refvelocity_norm[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    axes[1,i].imshow(predv2_norm[plt_t,:,:,i*plt_zgap,0].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist2 = {'loss':[], 'plane':[], 'div':[], 'momentum':[]}\n",
    "best_l = np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "batch = 100\n",
    "for i in range(n_epochs):\n",
    "    [key] = jax.random.split(key,1)\n",
    "    l_epoch = {'loss':[], 'div':[], 'plane':[], 'momentum':[]}\n",
    "    j = 0\n",
    "    while (j+1)*batch <= n_train :\n",
    "        _inns2 = (\n",
    "            jnp.einsum('txz -> tzx', inns_norm[j*batch:(j+1)*batch,...]), \n",
    "            jnp.reshape(z_coord, (20,1))\n",
    "        )\n",
    "        (_l, _l_components), state2 = update2(state2, key, _inns2, u_train_norm[j*batch:(j+1)*batch,...])\n",
    "        j += 1\n",
    "        l_epoch['loss'].append(float(_l))\n",
    "        for k,a in _l_components.items():\n",
    "            l_epoch[k].append(float(a))\n",
    "    for k,a in l_epoch.items():\n",
    "        hist2[k].append(np.mean(a))\n",
    "    if i % 20 == 0:\n",
    "        print(f'epoch {i}, ', \" \".join(f\"{k}: {a[-1]:.5f}\" for k, a in hist2.items()))\n",
    "    if hist2['loss'][-1] < best_l:\n",
    "        best_state2 = state2\n",
    "        best_l = hist2['loss'][-1]\n",
    "plt.figure(figsize=(5,3))\n",
    "for k in hist2.keys():\n",
    "    plt.semilogy(hist2[k], label=k)\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss mse')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predv2_norm = mdl2.apply(\n",
    "    hk.data_structures.merge(best_state2.params,params2_non_trainable),\n",
    "    None, \n",
    "    *inns2_group)\n",
    "print(f'volume relative error {losses.relative_error(predv2_norm,u_train_norm)}, measured plane {losses.relative_error(predv2_norm[:,:,:,z_plane,:],u_train_norm[:,:,:,z_plane,:])}')\n",
    "for i in range(0,z_plane+1):\n",
    "    print(losses.relative_error(predv2_norm[:,:,:,i,:],u_train_norm[:,:,:,i,:]))\n",
    "predv2 = unnormalise_group(predv2_norm, data_range=all_range, axis_data=-1, axis_range=0)\n",
    "print(f'Divergence {losses.divergence(predv2[...,:-1],datainfo)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_zgap = 2\n",
    "plt_t = 100\n",
    "which_component = 2\n",
    "vmin = u_train_norm[plt_t,...,::plt_zgap,which_component].min()\n",
    "vmax = u_train_norm[plt_t,...,::plt_zgap,which_component].max()\n",
    "fig,axes = plt.subplots(3,6,figsize=(15,6),height_ratios=(0.4,0.4,0.2))\n",
    "fig.suptitle(f'Normalized results. mdl1+correction. Top: ref, middle: pred, snapshot interval={plt_step}')\n",
    "for i in range(6):\n",
    "    axes[2,i].plot(inns_norm[plt_t,:,i*plt_zgap])\n",
    "    axes[2,i].set(xlabel=f'y at z={i*plt_zgap}, t={plt_t}', ylim=[inns_norm[plt_t,:,::plt_zgap].min(), inns_norm[plt_t,:,::plt_zgap].max()])\n",
    "    axes[0,i].imshow(u_train_norm[plt_t,:,:,i*plt_zgap,which_component].T, vmin=vmin, vmax=vmax)\n",
    "    axes[0,i].set(xlabel='x')\n",
    "    axes[1,i].imshow(predv2_norm[plt_t,:,:,i*plt_zgap,which_component].T, vmin=vmin, vmax=vmax)\n",
    "    axes[1,i].set(xlabel='x')\n",
    "axes[0,0].set_ylabel('y')\n",
    "axes[1,0].set_ylabel('y')\n",
    "axes[2,0].set_ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_t = 170\n",
    "which_component = 0\n",
    "fig, axes = plt.subplots(2, 2, figsize=(5,3))\n",
    "imref = axes[0,0].imshow(np.mean(u_train_norm[plt_t,:,:,:,which_component], axis=2).T)\n",
    "vmin, vmax = imref.get_clim()\n",
    "axes[1,0].imshow(np.mean(predv2_norm[plt_t,:,:,:,which_component], axis=2).T, vmax=vmax, vmin=vmin)\n",
    "\n",
    "imref = axes[0,1].imshow(np.mean(u_train_norm[:,:,:,:,which_component], axis=(0,3)).T)\n",
    "vmin, vmax = imref.get_clim()\n",
    "axes[1,1].imshow(np.mean(predv2_norm[:,:,:,:,which_component], axis=(0,3)).T, vmax=vmax, vmin=vmin)\n",
    "fig.suptitle('Top: ref, left: x-y plane averaged over z, right: x-y plane averaged over time and z')\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1,4, figsize=(7,2))\n",
    "for i,ax in enumerate(axes):\n",
    "    counts_true,bins_true = np.histogram(u_train_norm[...,i].flatten()-np.mean(u_train_norm[...,i].flatten()), density=True, bins='auto')\n",
    "    ax.stairs(counts_true,bins_true,label='true',linewidth=3, color='#808080',alpha=0.5)\n",
    "    counts,bins= np.histogram(predv2_norm[...,i].flatten()-np.mean(predv2_norm[...,i].flatten()), density=True, bins='auto')\n",
    "    ax.stairs(counts,bins,label='pred')\n",
    "axes[0].legend()\n",
    "fig.suptitle('u1,u2,u3,p')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
